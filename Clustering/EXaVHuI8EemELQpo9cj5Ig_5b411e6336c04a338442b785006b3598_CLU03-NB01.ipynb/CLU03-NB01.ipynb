{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-means with text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will\n",
    "* Cluster Wikipedia documents using k-means\n",
    "* Explore the role of random initialization on the quality of the clustering\n",
    "* Explore how results differ after changing the number of clusters\n",
    "* Evaluate clustering, both quantitatively and qualitatively\n",
    "\n",
    "When properly executed, clustering uncovers valuable insights from a set of unlabeled documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note to Amazon EC2 users**: To conserve memory, make sure to stop all the other notebooks before running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function # to conform python 2.x print to python 3.x\n",
    "import turicreate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data, extract features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with text data, we must first convert the documents into numerical features. As in the first assignment, let's extract TF-IDF features for each article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = turicreate.SFrame('people_wiki.sframe/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki['tf_idf'] = turicreate.text_analytics.tf_idf(wiki['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the remainder of the assignment, we will use sparse matrices. Sparse matrices are matrices that have a small number of nonzero entries. A good data structure for sparse matrices would only store the nonzero entries to save space and speed up computation. SciPy provides a highly-optimized library for sparse matrices. Many matrix operations available for NumPy arrays are also available for SciPy sparse matrices.\n",
    "\n",
    "We first convert the TF-IDF column (in dictionary format) into the SciPy sparse matrix format. We included plenty of comments for the curious; if you'd like, you may skip the next block and treat the function as a black box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sframe_to_scipy(x, column_name):\n",
    "    '''\n",
    "    Convert a dictionary column of an SFrame into a sparse matrix format where\n",
    "    each (row_id, column_id, value) triple corresponds to the value of\n",
    "    x[row_id][column_id], where column_id is a key in the dictionary.\n",
    "       \n",
    "    Example\n",
    "    >>> sparse_matrix, map_key_to_index = sframe_to_scipy(sframe, column_name)\n",
    "    '''\n",
    "    assert type(x[column_name][0]) == dict, \\\n",
    "        'The chosen column must be dict type, representing sparse data.'\n",
    "    \n",
    "    # 1. Add a row number (id)\n",
    "    x = x.add_row_number()\n",
    "\n",
    "    # 2. Stack will transform x to have a row for each unique (row, key) pair.\n",
    "    x = x.stack(column_name, ['feature', 'value'])\n",
    "\n",
    "    # Map feature words to integers \n",
    "    unique_words = sorted(x['feature'].unique())\n",
    "    mapping = {word:i for i, word in enumerate(unique_words)}\n",
    "    x['feature_id'] = x['feature'].apply(lambda x: mapping[x])\n",
    "\n",
    "    # Create numpy arrays that contain the data for the sparse matrix.\n",
    "    row_id = np.array(x['id'])\n",
    "    col_id = np.array(x['feature_id'])\n",
    "    data = np.array(x['value'])\n",
    "    \n",
    "    width = x['id'].max() + 1\n",
    "    height = x['feature_id'].max() + 1\n",
    "    \n",
    "    # Create a sparse matrix.\n",
    "    mat = csr_matrix((data, (row_id, col_id)), shape=(width, height))\n",
    "    return mat, mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 12s, sys: 7.52 s, total: 3min 19s\n",
      "Wall time: 3min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# The conversion will take about a minute or two.\n",
    "tf_idf, map_index_to_word = sframe_to_scipy(wiki, 'tf_idf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above matrix contains a TF-IDF score for each of the 59071 pages in the data set and each of the 547979 unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59071, 547979)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize all vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in the previous assignment, Euclidean distance can be a poor metric of similarity between documents, as it unfairly penalizes long articles. For a reasonable assessment of similarity, we should disregard the length information and use length-agnostic metrics, such as cosine distance.\n",
    "\n",
    "The k-means algorithm does not directly work with cosine distance, so we take an alternative route to remove length information: we normalize all vectors to be unit length. It turns out that Euclidean distance closely mimics cosine distance when all vectors are unit length. In particular, the squared Euclidean distance between any two vectors of length one is directly proportional to their cosine distance.\n",
    "\n",
    "We can prove this as follows. Let $\\mathbf{x}$ and $\\mathbf{y}$ be normalized vectors, i.e. unit vectors, so that $\\|\\mathbf{x}\\|=\\|\\mathbf{y}\\|=1$. Write the squared Euclidean distance as the dot product of $(\\mathbf{x} - \\mathbf{y})$ to itself:\n",
    "\\begin{align*}\n",
    "\\|\\mathbf{x} - \\mathbf{y}\\|^2 &= (\\mathbf{x} - \\mathbf{y})^T(\\mathbf{x} - \\mathbf{y})\\\\\n",
    "                              &= (\\mathbf{x}^T \\mathbf{x}) - 2(\\mathbf{x}^T \\mathbf{y}) + (\\mathbf{y}^T \\mathbf{y})\\\\\n",
    "                              &= \\|\\mathbf{x}\\|^2 - 2(\\mathbf{x}^T \\mathbf{y}) + \\|\\mathbf{y}\\|^2\\\\\n",
    "                              &= 2 - 2(\\mathbf{x}^T \\mathbf{y})\\\\\n",
    "                              &= 2(1 - (\\mathbf{x}^T \\mathbf{y}))\\\\\n",
    "                              &= 2\\left(1 - \\frac{\\mathbf{x}^T \\mathbf{y}}{\\|\\mathbf{x}\\|\\|\\mathbf{y}\\|}\\right)\\\\\n",
    "                              &= 2\\left[\\text{cosine distance}\\right]\n",
    "\\end{align*}\n",
    "\n",
    "This tells us that two **unit vectors** that are close in Euclidean distance are also close in cosine distance. Thus, the k-means algorithm (which naturally uses Euclidean distances) on normalized vectors will produce the same results as clustering using cosine distance as a distance metric.\n",
    "\n",
    "We import the [`normalize()` function](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html) from scikit-learn to normalize all vectors to unit length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "tf_idf = normalize(tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us implement the k-means algorithm. First, we choose an initial set of centroids. A common practice is to choose randomly from the data points.\n",
    "\n",
    "**Note:** We specify a seed here, so that everyone gets the same answer. In practice, we highly recommend to use different seeds every time (for instance, by using the current timestamp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_centroids(data, k, seed=None):\n",
    "    '''Randomly choose k data points as initial centroids'''\n",
    "    if seed is not None: # useful for obtaining consistent results\n",
    "        np.random.seed(seed)\n",
    "    n = data.shape[0] # number of data points\n",
    "        \n",
    "    # Pick K indices from range [0, N).\n",
    "    rand_indices = np.random.randint(0, n, k)\n",
    "    \n",
    "    # Keep centroids as dense format, as many entries will be nonzero due to averaging.\n",
    "    # As long as at least one document in a cluster contains a word,\n",
    "    # it will carry a nonzero weight in the TF-IDF vector of the centroid.\n",
    "    centroids = data[rand_indices,:].toarray()\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initialization, the k-means algorithm iterates between the following two steps:\n",
    "1. Assign each data point to the closest centroid.\n",
    "$$\n",
    "z_i \\gets \\mathrm{argmin}_j \\|\\mu_j - \\mathbf{x}_i\\|^2\n",
    "$$\n",
    "2. Revise centroids as the mean of the assigned data points.\n",
    "$$\n",
    "\\mu_j \\gets \\frac{1}{n_j}\\sum_{i:z_i=j} \\mathbf{x}_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pseudocode, we iteratively do the following:\n",
    "```\n",
    "cluster_assignment = assign_clusters(data, centroids)\n",
    "centroids = revise_centroids(data, k, cluster_assignment)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we implement Step 1 of the main k-means loop above? First import `pairwise_distances` function from scikit-learn, which calculates Euclidean distances between rows of given arrays. See [this documentation](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html) for more information.\n",
    "\n",
    "For the sake of demonstration, let's look at documents 100 through 102 as query documents and compute the distances between each of these documents and every other document in the corpus. In the k-means algorithm, we will have to compute pairwise distances between the set of centroids and the set of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.41000789 1.36894636]\n",
      " [1.40935215 1.41023886]\n",
      " [1.39855967 1.40890299]\n",
      " ...\n",
      " [1.41108296 1.39123646]\n",
      " [1.41022804 1.31468652]\n",
      " [1.39899784 1.41072448]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Get the TF-IDF vectors for documents 100 through 102.\n",
    "queries = tf_idf[100:102,:]\n",
    "\n",
    "# Compute pairwise distances from every data point to each query vector.\n",
    "dist = pairwise_distances(tf_idf, queries, metric='euclidean')\n",
    "\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More formally, `dist[i,j]` is assigned the distance between the `i`th row of `X` (i.e., `X[i,:]`) and the `j`th row of `Y` (i.e., `Y[j,:]`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** For a moment, suppose that we initialize three centroids with the first 3 rows of `tf_idf`. Write code to compute distances from each of the centroids to all data points in `tf_idf`. Then find the distance between row 430 of `tf_idf` and the second centroid and save it to `dist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.40979556]\n",
      " [1.40713107]\n",
      " [1.41002215]]\n"
     ]
    }
   ],
   "source": [
    "# Students should write code here\n",
    "dist=pairwise_distances(tf_idf[0:3,:],  tf_idf[430,:], metric='euclidean')\n",
    "print(dist)\n",
    "dist=dist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass\n"
     ]
    }
   ],
   "source": [
    "'''Test cell'''\n",
    "if np.allclose(dist, pairwise_distances(tf_idf[430,:], tf_idf[1,:])):\n",
    "    print('Pass')\n",
    "else:\n",
    "    print('Check your code again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Next, given the pairwise distances, we take the minimum of the distances for each data point. Fittingly, NumPy provides an `argmin` function. See [this documentation](http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.argmin.html) for details.\n",
    "\n",
    "Read the documentation and write code to produce a 1D array whose i-th entry indicates the centroid that is the closest to the i-th data point. Use the list of distances from the previous checkpoint and save them as `distances`. The value 0 indicates closeness to the first centroid, 1 indicates closeness to the second centroid, and so forth. Save this array as `closest_cluster`.\n",
    "\n",
    "**Hint:** the resulting array should be as long as the number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.41000789 1.36894636]\n",
      " [1.40935215 1.41023886]\n",
      " [1.39855967 1.40890299]\n",
      " ...\n",
      " [1.41108296 1.39123646]\n",
      " [1.41022804 1.31468652]\n",
      " [1.39899784 1.41072448]]\n",
      "[1. 0. 0. ... 1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Students should write code here\n",
    "queries = tf_idf[100:102,:]\n",
    "\n",
    "# Compute pairwise distances from every data point to each query vector.\n",
    "\n",
    "distances=pairwise_distances(tf_idf, queries, metric='euclidean')\n",
    "print(distances)\n",
    "closest_cluster=np.zeros(len(distances))\n",
    "for i in range(len(distances)):\n",
    "    closest_cluster[i]=np.argmin(distances[i])\n",
    "print(closest_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass\n"
     ]
    }
   ],
   "source": [
    "'''Test cell'''\n",
    "reference = [list(row).index(min(row)) for row in distances]\n",
    "if np.allclose(closest_cluster, reference):\n",
    "    print('Pass')\n",
    "else:\n",
    "    print('Check your code again')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Let's put these steps together.  First, initialize three centroids with the first 3 rows of `tf_idf`. Then, compute distances from each of the centroids to all data points in `tf_idf`. Finally, use these distance calculations to compute cluster assignments and assign them to `cluster_assignment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         1.40775177 1.38784582]\n",
      " [1.40775177 0.         1.39867641]\n",
      " [1.38784582 1.39867641 0.        ]\n",
      " ...\n",
      " [1.37070999 1.40978937 1.40616385]\n",
      " [1.35214578 1.41306211 1.40869799]\n",
      " [1.40799024 1.41353429 1.40903605]]\n",
      "[1. 2. 3. ... 1. 1. 1.]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot cast array data from dtype('float64') to dtype('int64') according to the rule 'safe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-89dd3db0427b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcluster_assignment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_assignment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_assignment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mbincount\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot cast array data from dtype('float64') to dtype('int64') according to the rule 'safe'"
     ]
    }
   ],
   "source": [
    "# Students should write code here\n",
    "dist=pairwise_distances(tf_idf,  tf_idf[0:3,:], metric='euclidean')\n",
    "print(dist)\n",
    "cluster_assignment=np.zeros(len(dist))\n",
    "for i in range(len(dist)):\n",
    "    cluster_assignment[i]=np.argmin(dist[i])+1\n",
    "print(cluster_assignment)\n",
    "print(np.bincount(cluster_assignment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check your code again.\n"
     ]
    }
   ],
   "source": [
    "if len(cluster_assignment)==59071 and \\\n",
    "   np.array_equal(np.bincount(cluster_assignment), np.array([23061, 10086, 25924])):\n",
    "    print('Pass') # count number of data points for each cluster\n",
    "else:\n",
    "    print('Check your code again.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to fill in the blanks in this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_clusters(data, centroids):\n",
    "    \n",
    "    # Compute distances between each data point and the set of centroids:\n",
    "    # Fill in the blank (RHS only)\n",
    "    distances_from_centroids = pairwise_distances(data,centroids, metric='euclidean')  # YOUR CODE HERE\n",
    "    \n",
    "    # Compute cluster assignments for each data point:\n",
    "    # Fill in the blank (RHS only)\n",
    "    cluster_assignment=np.zeros(len(distances_from_centroids))\n",
    "    for i in range(len(distances_from_centroids)):\n",
    "        cluster_assignment[i]=np.argmin(distances_from_centroids[i])\n",
    "    print(cluster_assignment)\n",
    "    return cluster_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**. For the last time, let us check if Step 1 was implemented correctly. With rows 0, 2, 4, and 6 of `tf_idf` as an initial set of centroids, we assign cluster labels to rows 0, 10, 20, ..., and 90 of `tf_idf`. The resulting cluster labels should be `[0, 1, 1, 0, 0, 2, 0, 2, 2, 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 1. 0. 0. 2. 0. 2. 2. 1.]\n",
      "Pass\n"
     ]
    }
   ],
   "source": [
    "if np.allclose(assign_clusters(tf_idf[0:100:10], tf_idf[0:8:2]), np.array([0, 1, 1, 0, 0, 2, 0, 2, 2, 1])):\n",
    "    print('Pass')\n",
    "else:\n",
    "    print('Check your code again.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Revising clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn to Step 2, where we compute the new centroids given the cluster assignments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SciPy and NumPy arrays allow for filtering via Boolean masks. For instance, we filter all data points that are assigned to cluster 0 by writing\n",
    "```\n",
    "data[cluster_assignment==0,:]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To develop intuition about filtering, let's look at a toy example consisting of 3 data points and 2 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[1., 2., 0.],\n",
    "                 [0., 0., 0.],\n",
    "                 [2., 2., 0.]])\n",
    "centroids = np.array([[0.5, 0.5, 0.],\n",
    "                      [0., -0.5, 0.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assign these data points to the closest centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0.]\n",
      "[0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "cluster_assignment = assign_clusters(data, centroids)\n",
    "print(cluster_assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression `cluster_assignment==1` gives a list of Booleans that says whether each data point is assigned to cluster 1 or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_assignment==1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise for cluster 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True, False,  True])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_assignment==0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lieu of indices, we can put in the list of Booleans to pick and choose rows. Only the rows that correspond to a `True` entry will be retained.\n",
    "\n",
    "First, let's look at the data points (i.e., their values) assigned to cluster 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[cluster_assignment==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense since [0 0 0] is closer to [0 -0.5 0] than to [0.5 0.5 0].\n",
    "\n",
    "Now let's look at the data points assigned to cluster 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2., 0.],\n",
       "       [2., 2., 0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[cluster_assignment==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this makes sense since these values are each closer to [0.5 0.5 0] than to [0 -0.5 0].\n",
    "\n",
    "Given all the data points in a cluster, it only remains to compute the mean. Use [np.mean()](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.mean.html). By default, the function averages all elements in a 2D array. To compute row-wise or column-wise means, add the `axis` argument. See the linked documentation for details. \n",
    "\n",
    "Use this function to average the data points in cluster 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5, 2. , 0. ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[cluster_assignment==0].mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to complete this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revise_centroids(data, k, cluster_assignment):\n",
    "    new_centroids = []\n",
    "    for i in range(k):\n",
    "        \n",
    "        # Select all data points that belong to cluster i. Fill in the blank (RHS only)\n",
    "        member_data_points = data[cluster_assignment==i] \n",
    "#         print(cluster_assignment)# YOUR CODE HERE\n",
    "        # Compute the mean of the data points. Fill in the blank (RHS only)\n",
    "        centroid = member_data_points.mean(axis=0)   # YOUR CODE HERE\n",
    "#         print(centroid)\n",
    "        # Convert numpy.matrix type to numpy.ndarray type\n",
    "        centroid = centroid.A1\n",
    "        new_centroids.append(centroid)\n",
    "    new_centroids = np.array(new_centroids)\n",
    "    \n",
    "    return new_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**. Let's check our Step 2 implementation. Letting rows 0, 10, ..., 90 of `tf_idf` as the data points and the cluster labels `[0, 1, 1, 0, 0, 2, 0, 2, 2, 1]`, we compute the next set of centroids. Each centroid is given by the average of all member data points in corresponding cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass\n"
     ]
    }
   ],
   "source": [
    "result = revise_centroids(tf_idf[0:100:10], 3, np.array([0, 1, 1, 0, 0, 2, 0, 2, 2, 1]))\n",
    "if np.allclose(result[0], np.mean(tf_idf[[0,30,40,60]].toarray(), axis=0)) and \\\n",
    "   np.allclose(result[1], np.mean(tf_idf[[10,20,90]].toarray(), axis=0))   and \\\n",
    "   np.allclose(result[2], np.mean(tf_idf[[50,70,80]].toarray(), axis=0)):\n",
    "    print('Pass')\n",
    "else:\n",
    "    print('Check your code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we tell if the k-means algorithm is converging? We can look at the cluster assignments and see if they stabilize over time. In fact, we'll be running the algorithm until the cluster assignments stop changing at all. To be extra safe, and to assess the clustering performance, we'll be looking at an additional criteria: the sum of all squared distances between data points and centroids. This is defined as\n",
    "$$\n",
    "J(\\mathcal{Z},\\mu) = \\sum_{j=1}^k \\sum_{i:z_i = j} \\|\\mathbf{x}_i - \\mu_j\\|^2.\n",
    "$$\n",
    "The smaller the distances, the more homogeneous the clusters are. In other words, we'd like to have \"tight\" clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_heterogeneity(data, k, centroids, cluster_assignment):\n",
    "    \n",
    "    heterogeneity = 0.0\n",
    "    for i in range(k):\n",
    "        \n",
    "        # Select all data points that belong to cluster i. Fill in the blank (RHS only)\n",
    "        member_data_points = data[cluster_assignment==i, :]\n",
    "        \n",
    "        if member_data_points.shape[0] > 0: # check if i-th cluster is non-empty\n",
    "            # Compute distances from centroid to data points (RHS only)\n",
    "            distances = pairwise_distances(member_data_points, [centroids[i]], metric='euclidean')\n",
    "            squared_distances = distances**2\n",
    "            heterogeneity += np.sum(squared_distances)\n",
    "        \n",
    "    return heterogeneity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the cluster heterogeneity for the 2-cluster example we've been considering based on our current cluster assignments and centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.25"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_heterogeneity(data, 2, centroids, cluster_assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining into a single function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the two k-means steps have been implemented, as well as our heterogeneity metric we wish to monitor, it is only a matter of putting these functions together to write a k-means algorithm that\n",
    "\n",
    "* Repeatedly performs Steps 1 and 2\n",
    "* Tracks convergence metrics\n",
    "* Stops if either no assignment changed or we reach a certain number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the blanks\n",
    "def kmeans(data, k, initial_centroids, maxiter, record_heterogeneity=None, verbose=False):\n",
    "    '''This function runs k-means on given data and initial set of centroids.\n",
    "       maxiter: maximum number of iterations to run.\n",
    "       record_heterogeneity: (optional) a list, to store the history of heterogeneity as function of iterations\n",
    "                             if None, do not store the history.\n",
    "       verbose: if True, print how many data points changed their cluster labels in each iteration'''\n",
    "    centroids = initial_centroids[:]\n",
    "    prev_cluster_assignment = None\n",
    "    \n",
    "    for itr in range(maxiter):        \n",
    "        if verbose:\n",
    "            print(itr)\n",
    "        \n",
    "        # 1. Make cluster assignments using nearest centroids\n",
    "        # YOUR CODE HERE\n",
    "        cluster_assignment =assign_clusters(data,centroids)\n",
    "            \n",
    "        # 2. Compute a new centroid for each of the k clusters, averaging all data points assigned to that cluster.\n",
    "        # YOUR CODE HERE\n",
    "        centroids = revise_centroids(data, k, cluster_assignment)\n",
    "            \n",
    "        # Check for convergence: if none of the assignments changed, stop\n",
    "        if prev_cluster_assignment is not None and \\\n",
    "          (prev_cluster_assignment==cluster_assignment).all():\n",
    "            break\n",
    "        \n",
    "        # Print number of new assignments \n",
    "        if prev_cluster_assignment is not None:\n",
    "            num_changed = np.sum(prev_cluster_assignment!=cluster_assignment)\n",
    "            if verbose:\n",
    "                print('    {0:5d} elements changed their cluster assignment.'.format(num_changed))   \n",
    "        \n",
    "        # Record heterogeneity convergence metric\n",
    "        if record_heterogeneity is not None:\n",
    "            # YOUR CODE HERE\n",
    "            score = compute_heterogeneity(data, k, centroids, cluster_assignment)\n",
    "            record_heterogeneity.append(score)\n",
    "        \n",
    "        prev_cluster_assignment = cluster_assignment[:]\n",
    "        \n",
    "    return centroids, cluster_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting convergence metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the above function to plot the convergence metric across iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heterogeneity(heterogeneity, k):\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(heterogeneity, linewidth=4)\n",
    "    plt.xlabel('# Iterations')\n",
    "    plt.ylabel('Heterogeneity')\n",
    "    plt.title('Heterogeneity of clustering over time, K={0:d}'.format(k))\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider running k-means with K=3 clusters for a maximum of 400 iterations, recording cluster heterogeneity at every step.  Then, let's plot the heterogeneity over iterations using the plotting function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[1. 0. 0. ... 1. 1. 0.]\n",
      "1\n",
      "[1. 2. 0. ... 1. 1. 1.]\n",
      "    19157 elements changed their cluster assignment.\n",
      "2\n",
      "[1. 2. 0. ... 1. 1. 1.]\n",
      "     7739 elements changed their cluster assignment.\n",
      "3\n",
      "[1. 2. 0. ... 1. 1. 1.]\n",
      "     5119 elements changed their cluster assignment.\n",
      "4\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "     3370 elements changed their cluster assignment.\n",
      "5\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "     2811 elements changed their cluster assignment.\n",
      "6\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "     3233 elements changed their cluster assignment.\n",
      "7\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "     3815 elements changed their cluster assignment.\n",
      "8\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "     3172 elements changed their cluster assignment.\n",
      "9\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "     1149 elements changed their cluster assignment.\n",
      "10\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "      498 elements changed their cluster assignment.\n",
      "11\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "      265 elements changed their cluster assignment.\n",
      "12\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "      149 elements changed their cluster assignment.\n",
      "13\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "      100 elements changed their cluster assignment.\n",
      "14\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       76 elements changed their cluster assignment.\n",
      "15\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       67 elements changed their cluster assignment.\n",
      "16\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       51 elements changed their cluster assignment.\n",
      "17\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       47 elements changed their cluster assignment.\n",
      "18\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       40 elements changed their cluster assignment.\n",
      "19\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       34 elements changed their cluster assignment.\n",
      "20\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       35 elements changed their cluster assignment.\n",
      "21\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       39 elements changed their cluster assignment.\n",
      "22\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       24 elements changed their cluster assignment.\n",
      "23\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       16 elements changed their cluster assignment.\n",
      "24\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       12 elements changed their cluster assignment.\n",
      "25\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       14 elements changed their cluster assignment.\n",
      "26\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       17 elements changed their cluster assignment.\n",
      "27\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       15 elements changed their cluster assignment.\n",
      "28\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       14 elements changed their cluster assignment.\n",
      "29\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       16 elements changed their cluster assignment.\n",
      "30\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       21 elements changed their cluster assignment.\n",
      "31\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       22 elements changed their cluster assignment.\n",
      "32\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       33 elements changed their cluster assignment.\n",
      "33\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       35 elements changed their cluster assignment.\n",
      "34\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       39 elements changed their cluster assignment.\n",
      "35\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       36 elements changed their cluster assignment.\n",
      "36\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       36 elements changed their cluster assignment.\n",
      "37\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       25 elements changed their cluster assignment.\n",
      "38\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       27 elements changed their cluster assignment.\n",
      "39\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       25 elements changed their cluster assignment.\n",
      "40\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       28 elements changed their cluster assignment.\n",
      "41\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       35 elements changed their cluster assignment.\n",
      "42\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       31 elements changed their cluster assignment.\n",
      "43\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       25 elements changed their cluster assignment.\n",
      "44\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       18 elements changed their cluster assignment.\n",
      "45\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       15 elements changed their cluster assignment.\n",
      "46\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "       10 elements changed their cluster assignment.\n",
      "47\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "        8 elements changed their cluster assignment.\n",
      "48\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "        8 elements changed their cluster assignment.\n",
      "49\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "        8 elements changed their cluster assignment.\n",
      "50\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "        7 elements changed their cluster assignment.\n",
      "51\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "        8 elements changed their cluster assignment.\n",
      "52\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "        3 elements changed their cluster assignment.\n",
      "53\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "        3 elements changed their cluster assignment.\n",
      "54\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "        4 elements changed their cluster assignment.\n",
      "55\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "        2 elements changed their cluster assignment.\n",
      "56\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "        3 elements changed their cluster assignment.\n",
      "57\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "        3 elements changed their cluster assignment.\n",
      "58\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "        1 elements changed their cluster assignment.\n",
      "59\n",
      "[1. 2. 0. ... 1. 1. 2.]\n",
      "        1 elements changed their cluster assignment.\n",
      "60\n",
      "[1. 2. 0. ... 1. 1. 2.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAELCAYAAADqYO7XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deZwcVb338c93ZhKykA0IYUlCAMMqECCigLKJIrLq9bohBJWH63YVrvvVe0XFxwcVEbgqLkAQL6KgKCoKiIAisiQadtkDCUECIfs+M7/njzo96enpmenpnp7KTH/fL5pUnVr6V9U1/etzquqUIgIzMzPLT1PeAZiZmTU6J2MzM7OcORmbmZnlzMnYzMwsZ07GZmZmOXMyNjMzy5mTsVmNJE2VtEpS8wC81yRJf5K0UtL5fVhumqSQ1FLP+CqM5RJJ/5V3HPUm6XeSZuUdhw0OTsZWMUnzJR1dUna6pDsqXP4cST+uT3T5iYhnI2LLiGgDkHSbpDPq9HZnAi8BYyPi43V6j271x7ZFxAci4sv9FdPmoNyxHRHHRsQVOcQyW9K5ReN7S3peUp+OF0k/TsutkPRYHY9pw8nYBpHNoVa3GdgJeDgGaW89A9F6UG+D6TiUNAO4FfhKRFTckpJ8FZgWEWOBE4FzJR3Y3zFaEhF++VXRC5gPHF1SdjpwR9H4DsDPgReBp4GPpvI3ARuAjcAq4L5UPg64FHgeeA44F2guWvdfgAuAl9O0JuDzwDPAYuBHwLii9z8tTVsC/FdxzGnZzwBPpuk/A7ZK06YBAcwCniWrfX6uaL2VLNsCfAVoA9al7fwf4NvA+SX77dfAWd3s50OAe4Hl6d9DUvnstP82pHUfXWbZkcD5aR8sB+5IZR0xlvssgXOAH6fhEcCP03YuSzFMKrdtaf49gJvTZ/Qo8Pai9c4GvgvcAKwGjk5l56bpRwALgY+nz/N54L1Fy2+d9tWKFMe5FB1vZbb/ROChFPdtwJ6p/DPAtSXzXghcVM1xWLKe7o7t24AzyqxjGfBU+pxPBxakbZ9VtM4tgG+QHYsvAJcAIyv8O52d4j+I7Dg+ox/+9ndP++btta7Lr272cd4B+DV4XqVf4Kns9MKXI1nCmgv8NzAc2CV96RyTpp9D+sIvWv6XwPeA0cC2wD3AvxWtuxX4d7JENxJ4H/BEWveWwC+AK9P8e6Uvw9em9/9G+oIsJOOzgLuAyenL7nvAT9K0aWTJ6gfpffYD1hd9mVeybCHRdXwJp/GDgEVAUxrfBlgDTCqzj7cClgKnpm1+VxrfOk2fTUkyKFn+2+n9dwSayb7wtygTY6fPks7J+N/IEuCotI4DyZrFy23baLJk8t4U7wFkCWDvoniXA4eSHR8j6JqMW4EvAcOAN6d9MyFNvzq9RqXPdwHdJGNgN7KE/4a0rk+RHSvDyVoU1hRtRzNZcnlNNcdhmffu2H9FZR37qmgd703vfS5Zov12+nzeCKwEtkzzfwu4nux4GJM+j69W+Hc6G7iJ7IfDqWWm/4bsB0G5129K5v1O2m8B/K0Qn191+H7NOwC/Bs+L7At8Vckf7xo2JeNXA8+WLPNZ4PI03OkLi6y2tb74y40s+dyahk8vs75bgA8Vje9OlnBbyH4E/KRo2iiyGkshGT8CvL5o+vZFy05LXziTi6bfA7yzD8uWTcZFy78hDX8EuKGbfXwqcE9J2V+B09PwbLpJxmTJbi2wX5lppTHOp/tk/D7gTmDfMuvptG3AO4A/l8zzPeALRfH+qGR6xzaQJeO1hbhS2WLgNWRJayOwe9G0bmvGZC0hPyvZH88BR6TxO4DT0vAbgCerPQ7LvHfH/iu3r9I6Hi+atk/6PCYVlS0BZgAi+1Gxa9G0g4GnK/w7nU3WkvA0sE21f+9F62sm+4H7eWBYrevzq/xr0Jz7sM3GyRHxh8KIpNOBwoUdOwE7SFpWNH8z8Odu1rUTWQ3meUmFsiay2k/BgpJldiBrgi14hiwhTkrTOuaPiDWSlpS833WS2ovK2tKyBf8sGl5DVvuudNmeXAG8h6w59z1kTaTllG4faXzHCt5jG7Ka55MVxtSdK4EpwNWSxpM1WX8uIjaWmXcn4NUln3lLWkdB6WdYaklEtBaNF/b7xLSuno6HYp32XUS0S1rApn13FVmS/RHw7jRe2Ia+HofVeKFoeG2KsbSssN2jgLlF8Yjsb6lS3wZ2BW6WdFRELK026MguTLxD0nuADwIXVbsu656TsfWnBWS/3qd3Mz3KzL+e7Nd7a5n5yy2ziOzLs2AqWfPfC2TNjrsXJkgaSXbOsfj93hcRfyl9E0nTunn/apYtjRmyhPagpP2APcmaRcsp3T7ItvH3vcQHWfPwOrIv4ft6mXc12Rd+wXaFgZR0vwh8MW3bDWTngi+l/Gd4e0S8oYf3Krc/KvEi2Wc7GXgslU3pYf5FZDVOAJRlsilktWOAa4DzJU0G3kJW24TqjsO+Tu+Ll8gS894R8VxvM3ejDTgFuBa4UdLREbECsluugNd1s9yfI+LYbqa1kB1bVge+mtr60z3ACkmfljRSUrOkV0p6VZr+AjBNUhNARDxPdm7rfEljJTVJ2lXS4T28x0+AsyXtLGlL4P8CP01fotcCJ0g6RNJwsoSiomUvAb4iaScASRMlnVThtvVl2RfIzml3iIiFZBcgXQn8PCLWdrPsDcBukt4tqUXSO8jOlf6mtwAjoh24DPimpB3S/j9Y0hZlZp8HvFPSMEkzgbcVJkg6UtI+6crnFWRNxW3dbNtvUrynpnUNk/QqSXv2Fm8F29NGdk3AOZJGSdqD7AK97vwMOE7S6yUNI7sobD1ZkzsR8SJZ0/HlZD8aH0nl1RyHpTod27VIn+MPgAskbQsgaUdJxxTmSfeMH9HLejYC/0qW3G+QNDqVHxvZrXjlXsem9W8r6Z2StkzH0TFkrQp/rHX7rDwnY+s36cvzBLLzXk+TfQn8kOxKVchqJgBLJP0tDZ9GdoHNw2QXKl1Ldj62O5eRJbQ/pfdYR3ZhDRHxUBq+mqyWvJLs/OP6tOyFZBfF3CRpJdkFWa+ucPP6suyFwNskLZVU3KR3BVnN7cryi0FELAGOJ0skS8guQjo+Il6qMM5PAA+QJf6XgfMo/3f+X2S1nKVkP1quKpq2HdnnsILsXPftZDX7LtsWESvJLj56J1nN9J/pPcv9AKjGR8iOn3+S7befsOnz7CQiHiU7BXAx2bF3AnBCRGwomu0qsiu6rypZvK/HYalyx3YtPk128dldklYAfyC1+qSa/Sqyz7lHadvfSvZ38uvUWlSJIGuSXki2P75BdvX/r/q4HVYhRfRn64rZ5iPVnJcB0yPi6c0gnsPIktq0VPuxPpJ0HrBdRMzKO5a8pHO3e0fEZ/OOxfqPzxnbkCLpBLIrrkX2a/4BsiuHc5WaTT8G/NCJuHKpaXo42ef4KuD9bLpgsCFFxJDrxc7cTG1Dz0lkzaWLgOlktybl2vyTzp8uI2v2/FaesQxCY8jOG68mOyd8PuCmUhty3ExtZmaWM9eMzczMctZw54y32WabmDZtWt5hmJlZA5o7d+5LETGxtLzhkvG0adOYM2dO3mGYmVkDklTawx7gZmozM7PcORmbmZnlzMnYzMwsZ07GZmZmOXMyNjMzy1nDXU1dq+VrN/Lc0rUsW7uB5Ws2MnnCKPaZPK73Bc3MzLrhZNxH18xZwLm/faRj/PRDpjkZm5lZTdxM3UfjRw3vNL587cacIjEzs6HCybiPxo8c1ml86ZoN3cxpZmZWGSfjPpowunMyXrbGNWMzM6uNk3EfjRvpZmozM+tfTsZ9NH6Um6nNzKx/ORn3Uek54+VrN9Le7mdCm5lZ9ZyM+6iluYkxW2y6IywCVq5rzTEiMzMb7JyMqzDOTdVmZtaPnIyrMKHkXuNlvojLzMxq4GRcBV/EZWZm/cnJuArjSi/i8r3GZmZWAyfjKnRppnbN2MzMauBkXIWuzdSuGZuZWfXqmowlzZf0gKR5kuakshmS7iqUSToolZ8k6f6i8tcWrWeWpMfTa1ZR+YFp/U9IukiS6rk9BV2aqX0Bl5mZ1WAgHqF4ZES8VDT+NeCLEfE7SW9O40cAtwDXR0RI2hf4GbCHpK2ALwAzgQDmSro+IpYC3wXOBO4CbgDeBPyu3hvkZmozM+tPeTRTBzA2DY8DFgFExKqIKHRlNTrNB3AMcHNEvJwS8M3AmyRtD4yNiL+m5X4EnDwQG+BmajMz60/1rhkHcJOkAL4XEd8HzgJulPQNsh8DhxRmlvQW4KvAtsBxqXhHYEHROhemsh3TcGl5F5LOJKtBM3Xq1Jo3qjQZ+z5jMzOrRb1rxodGxAHAscCHJR0GfBA4OyKmAGcDlxZmjojrImIPshrul1NxufPA0UN518KI70fEzIiYOXHixOq3Jhlf0ky93M3UZmZWg7om44goNEEvBq4DDgJmAb9Is1yTykqX+xOwq6RtyGq8U4omTyZr2l6YhkvL6670YRFupjYzs1rULRlLGi1pTGEYeCPwIFnCPDzNdhTweJrnFYWroSUdAAwHlgA3Am+UNEHShLSeGyPieWClpNek5U4DflWv7SlWejX1inUbafOTm8zMrEr1PGc8Cbgu5dcW4KqI+L2kVcCFklqAdaRzucC/AKdJ2gisBd6RLsx6WdKXgXvTfF+KiJfT8AeB2cBIsquo634lNaQnN41o6XhaU/bkpo1dmq/NzMwqUbdkHBFPAfuVKb8DOLBM+XnAed2s6zLgsjLlc4BX1hxsFcaPGtbp0YlL1zgZm5lZddwDV5XGj/S9xmZm1j+cjKvk25vMzKy/OBlXqbRJ2jVjMzOrlpNxlUpvb1rm25vMzKxKTsZVmlDaTO1kbGZmVXIyrtI4N1ObmVk/cTKuUpdmal/AZWZmVXIyrtKE0W6mNjOz/uFkXKVxvs/YzMz6iZNxlXyfsZmZ9Rcn4ypN6HIBl5OxmZlVx8m4SmNHdO7W209uMjOzajkZV6nw5KaCCFjhpmozM6uCk3ENujRVOxmbmVkVnIxrUHoR11JfUW1mZlVwMq7BuJKOP5b7Ii4zM6uCk3ENSpupXTM2M7NqOBnXoMu9xq4Zm5lZFZyMa+D+qc3MrD84GddgvJ/cZGZm/cDJuAZupjYzs/7gZFwD909tZmb9wcm4Bm6mNjOz/uBkXIMuF3C5mdrMzKrgZFwD14zNzKw/OBnXoLQHrhXrWmlta88pGjMzG6ycjGvQ3KQyj1JszSkaMzMbrJyMa+SmajMzq5WTcY0mdHlyky/iMjOzvnEyrtG4kprx8rWuGZuZWd84GdfItzeZmVmtnIxr5GZqMzOrlZNxjbo0U/sCLjMz6yMn4xr5MYpmZlYrJ+MaTRjtZmozM6uNk3GNxo/0fcZmZlYbJ+MajSu5gGu5m6nNzKyPnIxrNKHkAq6lrhmbmVkfORnXyPcZm5lZrSpKxpKa6x3IYDV25DCkTeMr/eQmMzPro0prxk9I+rqkveoazSCUPbnJ543NzKx6lSbjfYHHgB9KukvSmZLG1jGuQWX8KN9rbGZm1asoGUfEyoj4QUQcAnwK+ALwvKQrJL2irhEOAj5vbGZmtaj4nLGkEyVdB1wInA/sAvwauKGO8Q0KfqaxmZnVoqXC+R4HbgW+HhF3FpVfK+mw/g9rcOnSTO2asZmZ9UGlyfi0iLijuEDSoRHxl4j4aB3iGlTcP7WZmdWi0gu4LipTdnF/BjKYuZnazMxq0WPNWNLBwCHAREn/UTRpLOB7jxM3U5uZWS16a6YeDmyZ5htTVL4CeFu9ghpsSpOxu8Q0M7O+6DEZR8TtwO2SZkfEMwMU06BT2kztTj/MzKwvemum/lZEnAX8j6QonR4RJ9YtskHE9xmbmVktemumvjL9+416BzKY+clNZmZWi96aqeemf2+XNBKYGhGPDkhkg0jpOePlrhmbmVkfVNoD1wnAPOD3aXyGpOvrGdhgMmZEyZOb1rey0U9uMjOzClV6n/E5wEHAMoCImAdMq09Ig09zkxg30k9uMjOz6lSajFsjYnldIxnkfBGXmZlVq9Jk/KCkdwPNkqZLuhi4s7eFJM2X9ICkeZLmpLIZ6TGM8yTNkXRQKj9F0v3pdaek/YrW8yZJj0p6QtJnisp3lnS3pMcl/VTS8K5RDIxxXW5v8kVcZmZWmUqT8b8DewPrgZ+QdfpxVoXLHhkRMyJiZhr/GvDFiJgB/HcaB3gaODwi9gW+DHwfsidGAd8GjgX2At4laa+0zHnABRExHVgKvL/CmPrdhNKOP1a7ZmxmZpWp9HnGayLicxHxqoiYmYbXVfmeQdadJsA4YFF6jzsjYmkqvwuYnIYPAp6IiKciYgNwNXCSJAFHAdem+a4ATq4yppr5YRFmZlatip7aJGk34BNkF211LBMRR/WyaAA3pQ5DvhcR3yerUd8o6RtkPwYOKbPc+4HfpeEdgQVF0xYCrwa2BpZFRGtR+Y7dxH8mcCbA1KlTewm5On5YhJmZVavSRyheA1wC/BBo68P6D42IRZK2BW6W9A+yPq3PjoifS3o7cClwdGEBSUeSJePXForKrDd6KO9amP0I+D7AzJkzy85TKz8swszMqlVpMm6NiO/2deURUWiCXizpOrIm51nAx9Is15AleAAk7ZvGj42IJal4ITClaLWTyZq2XwLGS2pJteNCeS66NlO7ZmxmZpWp9AKuX0v6kKTtJW1VePW0gKTRksYUhoE3Ag+SJczD02xHAY+neaYCvwBOjYjHilZ1LzA9XTk9HHgncH1EBHArm54eNQv4VYXb0++6NlO7ZmxmZpWptGY8K/37yaKyAHbpYZlJwHXZdVa0AFdFxO8lrQIulNQCrCOdyyW7snpr4DtpmdZ0sVirpI8AN5I9Q/myiHgoLfNp4GpJ5wJ/J2vyzoWbqc3MrFoVJeOI2LmvK46Ip4D9ypTfARxYpvwM4Ixu1nUDcEM373FQX2Orhy41YzdTm5lZhSrtm3qUpM9LKtz7O13S8fUNbXBxD1xmZlatSs8ZXw5sYNNtSAuBc+sS0SBV+hhFJ2MzM6tUpcl414j4GrARICLWUv7WooY1ZkRLpyc3rfKTm8zMrEKVJuMN6XnGASBpV7KuMS1palKX2vHCpWtzisbMzAaTSpPxF8ieZTxF0v8CtwCfqltUg9Qe243pND5vwdJu5jQzM9uk0r6pbwbeCpxO9qCImRFxW/3CGpz2nzq+0/i8Z5flFImZmQ0mlfZNfUAafD79O1XSOOCZor6hG96MKRM6jf99gZOxmZn1rtJOP74DHADcT3bh1ivT8NaSPhARN9UpvkFlxpTONeOHF61g3cY2RgxrzikiMzMbDCo9Zzwf2D/1iHUgsD9Z15ZHs+l5xA1v4pgtmDxhZMd4a3vw0KLlOUZkZmaDQaXJeI+iLiiJiIfJkvNT9Qlr8Np/aklTtc8bm5lZLypNxo9K+q6kw9PrO8BjkrYg3Xtsmf1Lmqp93tjMzHpTaTI+HXgCOAs4G3gqlW0EjqxHYIPVDF9RbWZmfVTpgyLWSroYuIms449HI6JQI15Vr+AGo713GMvw5iY2pN63nlu2lsUr1rHt2BE5R2ZmZpurSh8UcQTZc4f/h+zK6sckHVbHuAatLVqa2WuHsZ3K3FRtZmY9qbSZ+nzgjRFxeEQcBhwDXFC/sAa30lucfBGXmZn1pNJkPCwiHi2MRMRjwLAe5m9oXXricreYZmbWg0o7/Zgj6VLgyjR+CjC3PiENfvuX9MR1/8LltLUHzU1+0JWZmXVVac34g8BDwEeBjwEPAx+oV1CD3ZStRrL16E1PcFqzoY3HXliZY0RmZrY56zUZS2oGLo2Ib0bEWyPiLRFxQUT4EYrdkNSlqdrnjc3MrDu9JuOIaAMmShre27y2SdeLuHze2MzMyqv0nPF84C+SrgdWFwoj4pv1CGooKO0Wc55vbzIzs25UmowXpVcTMKZ+4Qwd+04ehwQR2fgTL65ixbqNjB3hi9DNzKyzSnvg+iKApNERsbq3+Q3GjBjG9G235LEXsg7KIuD+Bct57fRtco7MzMw2N5X2wHWwpIeBR9L4fulhEdaD0lucfN7YzMzKqfTWpm+R9bq1BCAi7gPcHWYvSh8a4W4xzcysnEqTMRGxoKSorZ9jGXK69sS1jCicRDYzM0sqTcYLJB0ChKThkj5BarK27k3fdgyjhzd3jL+8egPPvrwmx4jMzGxzVGky/gDwYWBHYCEwA/hQvYIaKpqbxL6Tu9aOzczMilWajHePiFMiYlJEbBsR7wH2rGdgQ4V74jIzs95UmowvrrDMSnTpics1YzMzK9HjfcaSDgYOIesO8z+KJo0FmssvZcVKr6h+eNFy1m1sY8Qw7z4zM8v0VjMeDmxJlrTHFL1WAG+rb2hDw7ZjRjB5wsiO8Y1twUOLVuQYkZmZbW56rBlHxO3A7ZJmR8Qz7oGrOjOmjGfh0rUd43998iUO3GlCD0uYmVkjqfSc8Q7ugat6B+28Vafxn9yzgNa29pyiMTOzzY174BoAJ+63AyOGbdrVzy1byx8eeSHHiMzMbHPiHrgGwPhRw3nL/jt2Krv8L/PzCcbMzDY77oFrgMw6ZFqn8buffpmHfSGXmZlRWw9cH65XUEPRHtuN5eBdtu5UdsWd8/MJxszMNisVJeOIeKm0B66IWFLv4Iaa0w+d1mn8l/Oe4+XVG/IJxszMNhu9dfpxMdDtY4Yi4qP9HtEQdvSek9hx/EieW5bd5rS+tZ2r732WDx3xipwjMzOzPPVWM54DzE2vE4uGCy/rg+YmMeuQnTqVXfnXZ3ybk5lZg+ut048rCsOSzioet+q8Y+ZULrj5cdZuzC5Gf375Om56+AXevM/2OUdmZmZ5qfjWJnporrbKjRs1jLcc0Pk2p9m+zcnMrKH1JRlbP5l18LRO4/fMf5kHn1ueTzBmZpa7HpOxpJWSVkhaAexbGC6UD1CMQ87u243hkF19m5OZmWV6TMYRMSYixqZXS9HwmIgYO1BBDkWnl3QC8qv7FrFk1fp8gjEzs1y5mTonr99zUqdHK25obeequ5/NMSIzM8uLk3FOmpvU5dzxRX98nFv/sTifgMzMLDdOxjl6+8wpjB7e3DG+sS34tx/P5S9PvJRjVGZmNtCcjHM0btQwvvKWfTqVbWht54wr5nDv/JdzisrMzAaak3HOTt5/R849+ZWdytZubOO9l9/LfQuW5RSVmZkNJCfjzcB7XrMTnz9uz05lq9a3ctpl9/gxi2ZmDcDJeDNxxut24ZPH7N6pbPnajZx66d08/sLKnKIyM7OB4GS8Gfnwka/gI0d2foLTktUbeOf37/JFXWZmQ5iT8Wbm42/cjfe/dudOZUtWb+DUS+/m4lsep73dXYSbmQ01dU3GkuZLekDSPElzUtkMSXcVyiQdlMr3kPRXSeslfaJkPW+S9KikJyR9pqh8Z0l3S3pc0k8lDa/n9gwESXz+uD055dVTO5W3B5x/82O8d/a9vLx6Q07RmZlZPQxEzfjIiJgRETPT+NeAL0bEDOC/0zjAy8BHgW8ULyypGfg2cCywF/AuSXulyecBF0TEdGAp8P66bskAkcSXT3olH3v9dKTO025/7EWOu+jPzH1maT7BmZlZv8ujmTqAQr/W44BFABGxOCLuBTaWzH8Q8EREPBURG4CrgZMkCTgKuDbNdwVwcr2DHyhNTeLsN+zGFe89iK1Gd67wP798He/43l+59I6niXCztZnZYFfvZBzATZLmSjozlZ0FfF3SArJa8Gd7WceOwIKi8YWpbGtgWUS0lpR3IenM1CQ+58UXX6xyU/Jx2G4T+e1HX8uBO03oVN7aHnz5Nw/zxV8/7IRsZjbI1TsZHxoRB5A1MX9Y0mHAB4GzI2IKcDZwaS/rUJmy6KG8a2HE9yNiZkTMnDhxYuXRbya2HzeSq898Df/ndTt3mTb7zvmc9/tHnZDNzAaxuibjiOhoggauI2tyngX8Is1yTSrryUJgStH4ZLKm7ZeA8ZJaSsqHpGHNTXzuuL343qkHMmZES6dpl9z+JBfd8kROkZmZWa3qlowljZY0pjAMvBF4kCxhHp5mOwp4vJdV3QtMT1dODwfeCVwfWVXwVuBtab5ZwK/6dys2P8fsvR3XfOBgxo8a1qn8gj88xvdufzKnqMzMrBYtvc9StUnAddl1VrQAV0XE7yWtAi5MNdp1wJkAkrYD5pBd3NUu6Sxgr4hYIekjwI1AM3BZRDyU3uPTwNWSzgX+Tu9N3kPCHtuN5cr3vZp3/+AuVq5v7Sj/6u/+wRYtTZx+aNfmbDMz23yp0c41zpw5M+bMmZN3GP1i7jMvc+ql97BmQ1un8q++dR/eddDUbpYyM7O8SJpbdKtvB/fANYgduNNWXDrrVWzR0vlj/M/rHuCXf38up6jMzKyvnIwHuYN33ZofnDaT4c2bPsoI+NTP72fh0jU5RmZmZpVyMh4CDtttIt855QBamjbd7bWhtZ1v3+orrM3MBgMn4yHi6L0m8YUT9+5Uds2chTy7xLVjM7PNnZPxEPKuV01hp61HdYy3tgcX/bG3O8fMzCxvTsZDSEtzEx97/fROZb/420Kefml1ThGZmVklnIyHmJNm7MguE0d3jLcHXPiHx3KMyMzMeuNkPMQ0N4mzjt6tU9mv7lvEE4tX5hSRmZn1xsl4CDp+n+3ZbdKWHeMRcMEffO7YzGxz5WQ8BDU1ibNLase/vf95/vHPFTlFZGZmPXEyHqKO2Xs79tp+bKeyC272uWMzs82Rk/EQ1dQkzn5D59rxjQ+9wIPPLc8pIjMz646T8RB29J7bsu/kcZ3KvuUrq83MNjtOxkOY1LV2/IdHFjNvwbKcIjIzs3KcjIe4I3abyP5Tx3cqu+LO+fkEY2ZmZTkZD3GSuvTKdfPDL7BuY1s3S5iZ2UBzMm4Ar5s+kYljtugYX7W+ldsefTHHiMzMrJiTcQNobhJvfuV2ncp+c/+inKIxM7NSTsYN4vj9dug0fssji1mzoTWnaMzMrJiTcYM4cOoEth83omN87cY2bnlkcY4RmZlZgZNxg2hqEsfts32nMjdVm5ltHpyMG0hpU/Wtj77IynUbc4rGzMwKnIwbyH6TxzFlq5Ed4xta2/nDIy/kGJGZmbd02+8AAAnuSURBVIGTcUORxPH7dq4d//q+53OKxszMCpyMG8zx+3Y+b/znx19k+Ro3VZuZ5cnJuMHstf1YdtlmdMf4xrbgxof+mWNEZmbmZNxgsqbqzrXjX/uqajOzXDkZN6ATSq6qvvPJJSxZtT6naMzMzMm4AU2fNIbdJ43pGG9rD373oJuqzczy4mTcoEqbqt0BiJlZfpyMG1RpByB3P/0yi1esyykaM7PG5mTcoHbeZjR77zC2YzwCfvuA7zk2M8uDk3EDK72Q67u3PcnP7l3Axrb2nCIyM2tMTsYNrPTBEYtXrudTP7+fI75+Gz++6xnWt7blFJmZWWNxMm5gU7YaxWG7TexS/tyytXz+lw9y+Ndu4/K/PM1jL6xk4dI1LF29gfWtbUREDtGamQ1darQv1pkzZ8acOXPyDmOzsXzNRr76u0e4du5CWtsrOxZamsSo4c1sMayZlibR0ixamppoaRLNabxZAokmQVP6VxICJBAizdIxDNk8QMd8heHCtE3DhWjUabzr9Gz9lCkv1d0y9LRMt+sqP6WHVVWlp+3p87qqev++L9XtEj3u5272ZzfL9BRV98tUsS0VHk+l79TndVUcUW3683jaXFXzOQPssf0YTnn1TrW/vzQ3ImaWlrfUvGYb1MaNGsb/+5d9+chRr+CS25/kZ/cuZEMv54xb24MV61phXesARWlmlq837DWpX5Jxd9xMbQBMnjCKc0/ehz996kjee+g0tmjxoWFmNlBcM7ZOths3gi+csDcfOuIVzL7zae58cgkr17WyZn0rqze0sXp9a8XN2WZmVhknYytr4pgt+OQxe5SdtqG1ndXrW9nQ1k5re9Da8W/Q2t5Oa1sQQHsEEUEEtEfW7WYQpP+IgCDSv3RcGBYd/8umU5i3oyz9Wzw/m6ZvKikuKy7tqvN85Zfvsky36xqYHyv9+TbR497pv/fvbpmeP5vyU7tdpsfPrJt1VbMtPU3rdjv7/v4D9tO3Aa4fqmULp0wY1W9xlONkbH02vKWJ4S3D8w7DzGzI8IlBMzOznDkZm5mZ5czJ2MzMLGdOxmZmZjlzMjYzM8uZk7GZmVnOGq5vakkvAs/00+q2AV7qp3UNRt5+b38jbz94H3j7+779O0VElyf0NFwy7k+S5pTr8LtRePu9/Y28/eB94O3vv+13M7WZmVnOnIzNzMxy5mRcm+/nHUDOvP2NrdG3H7wPvP39xOeMzczMcuaasZmZWc6cjM3MzHLmZFwFSW+S9KikJyR9Ju94BoKkyyQtlvRgUdlWkm6W9Hj6d0KeMdaTpCmSbpX0iKSHJH0slTfEPpA0QtI9ku5L2//FVL6zpLvT9v9U0pB+tqakZkl/l/SbNN4w2y9pvqQHJM2TNCeVNcTxXyBpvKRrJf0jfRcc3F/7wMm4jyQ1A98GjgX2At4laa98oxoQs4E3lZR9BrglIqYDt6TxoaoV+HhE7Am8Bvhw+twbZR+sB46KiP2AGcCbJL0GOA+4IG3/UuD9OcY4ED4GPFI03mjbf2REzCi6t7ZRjv+CC4HfR8QewH5kx0K/7AMn4747CHgiIp6KiA3A1cBJOcdUdxHxJ+DlkuKTgCvS8BXAyQMa1ACKiOcj4m9peCXZH+GONMg+iMyqNDosvQI4Crg2lQ/Z7QeQNBk4DvhhGhcNtP3daIjjH0DSWOAw4FKAiNgQEcvop33gZNx3OwILisYXprJGNCkinocsWQHb5hzPgJA0DdgfuJsG2gepiXYesBi4GXgSWBYRrWmWof638C3gU0B7Gt+axtr+AG6SNFfSmamsYY5/YBfgReDydKrih5JG00/7wMm471SmzPeHNQhJWwI/B86KiBV5xzOQIqItImYAk8laiPYsN9vARjUwJB0PLI6IucXFZWYdktufHBoRB5CdovuwpMPyDmiAtQAHAN+NiP2B1fRjs7yTcd8tBKYUjU8GFuUUS95ekLQ9QPp3cc7x1JWkYWSJ+H8j4hepuKH2AUBqmruN7Nz5eEktadJQ/ls4FDhR0nyyU1NHkdWUG2X7iYhF6d/FwHVkP8ga6fhfCCyMiLvT+LVkyblf9oGTcd/dC0xPV1EOB94JXJ9zTHm5HpiVhmcBv8oxlrpK5wcvBR6JiG8WTWqIfSBpoqTxaXgkcDTZefNbgbel2Ybs9kfEZyNickRMI/ub/2NEnEKDbL+k0ZLGFIaBNwIP0iDHP0BE/BNYIGn3VPR64GH6aR+4B64qSHoz2a/iZuCyiPhKziHVnaSfAEeQPTLsBeALwC+BnwFTgWeBf42I0ou8hgRJrwX+DDzApnOG/0l23njI7wNJ+5JdnNJM9iP+ZxHxJUm7kNUUtwL+DrwnItbnF2n9SToC+EREHN8o25+287o02gJcFRFfkbQ1DXD8F0iaQXYB33DgKeC9pL8HatwHTsZmZmY5czO1mZlZzpyMzczMcuZkbGZmljMnYzMzs5w5GZuZmeXMydhsCJD0VUlHSDq5uyeJSTpH0ifS8OmSdujH9z9C0iFF4x+QdFp/rd9sqHMyNhsaXk12z/PhZPdD9+Z0oE/JuKinqXKOADqScURcEhE/6sv6zRqZ7zM2G8QkfR04BtiZ7MENuwJPA9dGxJdK5j0HWAXMJ3sk5nPAWuBgsseBfhPYEngJOD0inpd0G3AnWXeQ1wOPAZ8n6/RgCXAKMBK4C2gj60j/38l6J1oVEd9IHSVcAoxKMb4vIpamdd8NHAmMB94fEX+WtDdweXqPJuBfIuLxftplZpsl14zNBrGI+CRwBllyfRVwf0TsW5qIS5a5FpgDnJIe/NAKXAy8LSIOBC4DinuVGx8Rh0fE+cAdwGtSR/lXA5+KiPlkyfaC9Kzb0pr5j4BPR8S+ZD2YfaFoWktEHAScVVT+AeDCFNtMsj6BzYa0npqdzGxw2B+YB+xB1lduX+0OvBK4OeuCm2bg+aLpPy0angz8NHWIP5ysFt4tSePIkvntqegK4JqiWQoP3JgLTEvDfwU+l54f/AvXiq0ROBmbDVKp+Xc2WYJ8iawZWOmZwwdHxNpKVwU8FBEHdzN9ddHwxcA3I+L61EfzOVWEXqzQj3Mb6fsoIq6SdDdwHHCjpDMi4o81vo/ZZs3N1GaDVETMS025j5Gd8/0jcExqKu4tEa8ExqThR4GJkg6G7FGR6bxtOePIzjXDpifVlK6vOMblwFJJr0tFpwK3l85XLD2U4KmIuIjsPPW+vWyL2aDnZGw2iEmaCCyNiHZgj4iotJl6NnBJqkU3kz0G8DxJ95E1eR/SzXLnANdI+jNZbbzg18BbJM0rSrwFs4CvS7ofmAF0ez47eQfwYIptD7JzzmZDmq+mNjMzy5lrxmZmZjlzMjYzM8uZk7GZmVnOnIzNzMxy5mRsZmaWMydjMzOznDkZm5mZ5ez/A9/5y9g8lSbhAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "k = 3\n",
    "heterogeneity = []\n",
    "initial_centroids = get_initial_centroids(tf_idf, k, seed=0)\n",
    "centroids, cluster_assignment = kmeans(tf_idf, k, initial_centroids, maxiter=400,\n",
    "                                       record_heterogeneity=heterogeneity, verbose=True)\n",
    "plot_heterogeneity(heterogeneity, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**. (True/False) The clustering objective (heterogeneity) is non-increasing for this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**. Let's step back from this particular example. If the clustering objective (heterogeneity) would ever increase when running k-means, that would indicate: (choose one)\n",
    "\n",
    "1. k-means algorithm got stuck in a bad local minimum\n",
    "2. There is a bug in the k-means code\n",
    "3. All data points consist of exact duplicates\n",
    "4. Nothing is wrong. The objective should generally go down sooner or later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**. Which of the cluster contains the greatest number of data points in the end? Hint: Use [`np.bincount()`](http://docs.scipy.org/doc/numpy-1.11.0/reference/generated/numpy.bincount.html) to count occurrences of each cluster label.\n",
    " 1. Cluster #0\n",
    " 2. Cluster #1\n",
    " 3. Cluster #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "object too deep for desired array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-68fd45bb2da1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# print(cluster_assignment)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mbincount\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: object too deep for desired array"
     ]
    }
   ],
   "source": [
    "np.bincount(cluster_assignment.astype(int))\n",
    "# print(cluster_assignment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beware of local maxima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One weakness of k-means is that it tends to get stuck in a local minimum. To see this, let us run k-means multiple times, with different initial centroids created using different random seeds.\n",
    "\n",
    "**Note:** Again, in practice, you should set different seeds for every run. We give you a list of seeds for this assignment so that everyone gets the same answer.\n",
    "\n",
    "This may take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 0. 6. ... 1. 1. 0.]\n",
      "[3. 0. 5. ... 3. 3. 1.]\n",
      "[3. 0. 5. ... 3. 3. 0.]\n",
      "[3. 0. 5. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 0. 8. ... 3. 3. 0.]\n",
      "[3. 0. 8. ... 3. 3. 0.]\n",
      "[3. 0. 8. ... 3. 3. 0.]\n",
      "[3. 0. 8. ... 3. 3. 0.]\n",
      "[3. 0. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "[3. 2. 8. ... 3. 3. 0.]\n",
      "seed=000000, heterogeneity=57457.52442, cluster_distribution=[18047  3824  5671  6983  1492  1730  3882  3449  7139  6854]\n",
      "[5. 3. 8. ... 0. 8. 8.]\n",
      "[7. 4. 7. ... 7. 7. 4.]\n",
      "[7. 4. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "[7. 5. 9. ... 7. 7. 4.]\n",
      "seed=020000, heterogeneity=57533.20100, cluster_distribution=[ 3142   768  3566  2277 15779  7278  6146  7964  6666  5485]\n",
      "[4. 6. 0. ... 4. 4. 6.]\n",
      "[4. 9. 0. ... 4. 4. 3.]\n",
      "[4. 9. 0. ... 4. 4. 3.]\n",
      "[4. 9. 0. ... 4. 4. 3.]\n",
      "[4. 9. 0. ... 4. 4. 9.]\n",
      "[4. 9. 0. ... 4. 4. 9.]\n",
      "[4. 1. 0. ... 4. 4. 9.]\n",
      "[4. 1. 0. ... 4. 4. 9.]\n",
      "[4. 1. 0. ... 4. 4. 9.]\n",
      "[4. 1. 0. ... 4. 4. 9.]\n",
      "[4. 1. 0. ... 4. 4. 9.]\n",
      "[4. 1. 0. ... 4. 4. 9.]\n",
      "[4. 1. 0. ... 4. 4. 9.]\n",
      "[4. 1. 0. ... 4. 4. 9.]\n",
      "[4. 1. 0. ... 4. 4. 9.]\n",
      "[4. 1. 0. ... 4. 4. 9.]\n",
      "[4. 1. 0. ... 4. 4. 9.]\n",
      "[4. 1. 0. ... 4. 4. 9.]\n",
      "[4. 1. 0. ... 4. 4. 9.]\n",
      "[4. 1. 0. ... 4. 4. 9.]\n",
      "[4. 1. 0. ... 4. 4. 9.]\n",
      "[4. 1. 0. ... 4. 4. 9.]\n",
      "[4. 1. 0. ... 4. 4. 9.]\n",
      "[4. 1. 0. ... 4. 4. 9.]\n",
      "[4. 1. 0. ... 4. 4. 9.]\n",
      "[4. 1. 0. ... 4. 4. 9.]\n",
      "[4. 1. 0. ... 4. 4. 9.]\n",
      "[4. 1. 0. ... 4. 4. 9.]\n",
      "[4. 1. 0. ... 4. 4. 9.]\n",
      "[4. 1. 0. ... 4. 4. 9.]\n",
      "[4. 1. 0. ... 4. 4. 9.]\n",
      "[4. 1. 0. ... 4. 4. 9.]\n",
      "[4. 1. 0. ... 4. 4. 9.]\n",
      "seed=040000, heterogeneity=57512.69257, cluster_distribution=[ 5551  6623   186  2999  8487  3893  6807  2921  3472 18132]\n",
      "[4. 9. 3. ... 4. 4. 9.]\n",
      "[4. 9. 5. ... 4. 4. 9.]\n",
      "[4. 9. 7. ... 4. 4. 9.]\n",
      "[4. 9. 7. ... 4. 4. 9.]\n",
      "[4. 9. 5. ... 4. 4. 9.]\n",
      "[4. 9. 5. ... 4. 4. 9.]\n",
      "[4. 9. 5. ... 4. 4. 9.]\n",
      "[4. 9. 5. ... 4. 4. 9.]\n",
      "[4. 9. 5. ... 4. 4. 9.]\n",
      "[4. 9. 5. ... 4. 4. 9.]\n",
      "[4. 9. 5. ... 4. 4. 9.]\n",
      "[4. 9. 5. ... 4. 4. 9.]\n",
      "[4. 9. 5. ... 4. 4. 9.]\n",
      "[4. 9. 5. ... 4. 4. 9.]\n",
      "[4. 9. 5. ... 4. 4. 9.]\n",
      "[4. 9. 5. ... 4. 4. 9.]\n",
      "[4. 9. 5. ... 4. 4. 9.]\n",
      "[4. 9. 5. ... 4. 4. 9.]\n",
      "[4. 9. 5. ... 4. 4. 9.]\n",
      "[4. 9. 5. ... 4. 4. 9.]\n",
      "[4. 9. 5. ... 4. 4. 9.]\n",
      "[4. 9. 5. ... 4. 4. 9.]\n",
      "[4. 9. 5. ... 4. 4. 9.]\n",
      "[4. 9. 5. ... 4. 4. 9.]\n",
      "[4. 9. 5. ... 4. 4. 9.]\n",
      "[4. 9. 5. ... 4. 4. 9.]\n",
      "seed=060000, heterogeneity=57466.97925, cluster_distribution=[ 3014  3089  6681  3856  8080  7222  3424   424  5381 17900]\n",
      "[8. 6. 2. ... 8. 1. 4.]\n",
      "[8. 6. 2. ... 8. 8. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 6. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "[8. 0. 2. ... 8. 1. 0.]\n",
      "seed=080000, heterogeneity=57494.92990, cluster_distribution=[17582  1785  7215  3314  6285   809  5930  6791  5536  3824]\n",
      "[6. 3. 8. ... 2. 6. 5.]\n",
      "[2. 4. 8. ... 2. 2. 4.]\n",
      "[2. 4. 8. ... 2. 2. 4.]\n",
      "[2. 4. 8. ... 2. 2. 4.]\n",
      "[2. 4. 8. ... 2. 6. 4.]\n",
      "[2. 4. 8. ... 2. 6. 4.]\n",
      "[2. 4. 8. ... 2. 2. 4.]\n",
      "[2. 4. 8. ... 2. 2. 4.]\n",
      "[2. 4. 8. ... 2. 2. 4.]\n",
      "[2. 4. 8. ... 2. 2. 4.]\n",
      "[2. 4. 8. ... 2. 2. 4.]\n",
      "[2. 9. 8. ... 2. 2. 4.]\n",
      "[2. 9. 8. ... 2. 2. 4.]\n",
      "[2. 9. 8. ... 2. 2. 4.]\n",
      "[2. 9. 8. ... 2. 2. 4.]\n",
      "[2. 9. 8. ... 2. 2. 4.]\n",
      "[2. 9. 8. ... 2. 2. 4.]\n",
      "[2. 9. 8. ... 2. 2. 4.]\n",
      "[2. 9. 8. ... 2. 2. 4.]\n",
      "[2. 9. 8. ... 2. 2. 4.]\n",
      "[2. 9. 8. ... 2. 2. 4.]\n",
      "[2. 9. 8. ... 2. 2. 4.]\n",
      "[2. 9. 8. ... 2. 2. 4.]\n",
      "[2. 9. 8. ... 2. 2. 4.]\n",
      "[2. 9. 8. ... 2. 2. 4.]\n",
      "[2. 9. 8. ... 2. 2. 4.]\n",
      "[2. 9. 8. ... 2. 2. 4.]\n",
      "[2. 9. 8. ... 2. 2. 4.]\n",
      "[2. 9. 8. ... 2. 2. 4.]\n",
      "seed=100000, heterogeneity=57484.42210, cluster_distribution=[ 6618  1337  6191  2890 16969  4983  5242  3892  5562  5387]\n",
      "[3. 2. 0. ... 3. 2. 0.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 6. 8. ... 3. 3. 6.]\n",
      "[3. 6. 8. ... 3. 3. 6.]\n",
      "[3. 6. 8. ... 3. 3. 6.]\n",
      "[3. 6. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "[3. 2. 8. ... 3. 3. 6.]\n",
      "seed=120000, heterogeneity=57554.62410, cluster_distribution=[ 6118  5841  4964  8423  4302  3183 16481  1608  5524  2627]\n",
      "313.0903720855713\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "heterogeneity = {}\n",
    "cluster_assignment_dict = {}\n",
    "import time\n",
    "start = time.time()\n",
    "for seed in [0, 20000, 40000, 60000, 80000, 100000, 120000]:\n",
    "    initial_centroids = get_initial_centroids(tf_idf, k, seed)\n",
    "    centroids, cluster_assignment = kmeans(tf_idf, k, initial_centroids, maxiter=400,\n",
    "                                           record_heterogeneity=None, verbose=False)\n",
    "    # To save time, compute heterogeneity only once in the end\n",
    "    heterogeneity[seed] = compute_heterogeneity(tf_idf, k, centroids, cluster_assignment)\n",
    "\n",
    "    # This is the line we added for the next quiz question\n",
    "    cluster_assignment_dict[seed] = np.bincount(cluster_assignment.astype(int))\n",
    "    \n",
    "#    print('seed={0:06d}, heterogeneity={1:.5f}'.format(seed, heterogeneity[seed]))\n",
    "    # And this is the modified print statement\n",
    "    print('seed={0:06d}, heterogeneity={1:.5f}, cluster_distribution={2}'.format(seed, heterogeneity[seed], \n",
    "                                           cluster_assignment_dict[seed]))\n",
    "    sys.stdout.flush()\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the variation in heterogeneity for different initializations. This indicates that k-means sometimes gets stuck at a bad local minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**. Another way to capture the effect of changing initialization is to look at the distribution of cluster assignments. Add a line to the code above to compute the size (# of member data points) of clusters for each run of k-means. Look at the size of the largest cluster (most # of member data points) across multiple runs, with seeds 0, 20000, ..., 120000. How much does this measure vary across the runs? What is the minimum and maximum values this quantity takes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One effective way to counter this tendency is to use **k-means++** to provide a smart initialization. This method tries to spread out the initial set of centroids so that they are not too close together. It is known to improve the quality of local optima and lower average runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_initialize(data, k, seed=None):\n",
    "    '''Use k-means++ to initialize a good set of centroids'''\n",
    "    if seed is not None: # useful for obtaining consistent results\n",
    "        np.random.seed(seed)\n",
    "    centroids = np.zeros((k, data.shape[1]))\n",
    "    \n",
    "    # Randomly choose the first centroid.\n",
    "    # Since we have no prior knowledge, choose uniformly at random\n",
    "    idx = np.random.randint(data.shape[0])\n",
    "    centroids[0] = data[idx,:].toarray()\n",
    "    # Compute distances from the first centroid chosen to all the other data points\n",
    "    squared_distances = pairwise_distances(data, centroids[0:1], metric='euclidean').flatten()**2\n",
    "    \n",
    "    for i in range(1, k):\n",
    "        # Choose the next centroid randomly, so that the probability for each data point to be chosen\n",
    "        # is directly proportional to its squared distance from the nearest centroid.\n",
    "        # Roughtly speaking, a new centroid should be as far as from ohter centroids as possible.\n",
    "        idx = np.random.choice(data.shape[0], 1, p=squared_distances/sum(squared_distances))\n",
    "        centroids[i] = data[idx,:].toarray()\n",
    "        # Now compute distances from the centroids to all data points\n",
    "        squared_distances = np.min(pairwise_distances(data, centroids[0:i+1], metric='euclidean')**2,axis=1)\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now rerun k-means with 10 clusters using the same set of seeds, but always using k-means++ to initialize the algorithm.\n",
    "\n",
    "This may take several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 9. 6. ... 2. 2. 2.]\n",
      "[2. 0. 6. ... 2. 2. 2.]\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "seed=000000, heterogeneity=57468.63808\n",
      "[7. 3. 5. ... 7. 5. 7.]\n",
      "[7. 9. 5. ... 5. 5. 6.]\n",
      "[7. 9. 5. ... 5. 5. 6.]\n",
      "[5. 9. 5. ... 5. 5. 6.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "[5. 9. 0. ... 5. 4. 3.]\n",
      "seed=020000, heterogeneity=57486.94263\n",
      "[5. 4. 6. ... 1. 5. 5.]\n",
      "[5. 4. 7. ... 1. 5. 9.]\n",
      "[5. 4. 7. ... 1. 5. 9.]\n",
      "[5. 4. 7. ... 1. 5. 9.]\n",
      "[5. 4. 7. ... 1. 5. 9.]\n",
      "[5. 4. 7. ... 1. 5. 9.]\n",
      "[5. 4. 7. ... 1. 5. 9.]\n",
      "[5. 4. 7. ... 1. 5. 9.]\n",
      "[5. 4. 7. ... 1. 5. 9.]\n",
      "[5. 4. 7. ... 1. 5. 9.]\n",
      "[5. 4. 7. ... 1. 5. 0.]\n",
      "[5. 4. 7. ... 1. 5. 0.]\n",
      "[5. 4. 7. ... 1. 5. 0.]\n",
      "[5. 4. 7. ... 1. 5. 0.]\n",
      "[5. 4. 7. ... 1. 5. 0.]\n",
      "[5. 4. 7. ... 1. 5. 0.]\n",
      "[5. 4. 7. ... 1. 5. 0.]\n",
      "[5. 4. 7. ... 1. 5. 0.]\n",
      "[5. 4. 7. ... 1. 5. 0.]\n",
      "[5. 4. 7. ... 1. 5. 0.]\n",
      "[5. 4. 7. ... 1. 5. 0.]\n",
      "[5. 4. 7. ... 1. 5. 0.]\n",
      "[5. 4. 7. ... 1. 5. 0.]\n",
      "[5. 4. 7. ... 1. 5. 0.]\n",
      "[5. 4. 7. ... 1. 5. 0.]\n",
      "seed=040000, heterogeneity=57454.35926\n",
      "[2. 6. 2. ... 2. 2. 5.]\n",
      "[2. 1. 2. ... 2. 2. 9.]\n",
      "[2. 6. 2. ... 2. 2. 9.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "[2. 4. 1. ... 2. 2. 4.]\n",
      "seed=060000, heterogeneity=57530.43659\n",
      "[9. 2. 2. ... 9. 9. 5.]\n",
      "[9. 2. 6. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "[9. 5. 4. ... 9. 9. 5.]\n",
      "[9. 5. 4. ... 9. 9. 5.]\n",
      "[9. 5. 4. ... 9. 9. 5.]\n",
      "[9. 5. 4. ... 9. 9. 5.]\n",
      "[9. 5. 4. ... 9. 9. 5.]\n",
      "[9. 5. 4. ... 9. 9. 5.]\n",
      "[9. 5. 4. ... 9. 9. 5.]\n",
      "[9. 5. 4. ... 9. 9. 5.]\n",
      "[9. 5. 4. ... 9. 9. 5.]\n",
      "[9. 5. 4. ... 9. 9. 5.]\n",
      "[9. 5. 4. ... 9. 9. 5.]\n",
      "[9. 5. 4. ... 9. 9. 5.]\n",
      "[9. 5. 4. ... 9. 9. 5.]\n",
      "[9. 5. 4. ... 9. 9. 5.]\n",
      "[9. 5. 4. ... 9. 9. 5.]\n",
      "[9. 5. 4. ... 9. 9. 5.]\n",
      "[9. 5. 4. ... 9. 9. 5.]\n",
      "[9. 5. 4. ... 9. 9. 5.]\n",
      "[9. 5. 4. ... 9. 9. 5.]\n",
      "[9. 5. 4. ... 9. 9. 5.]\n",
      "[9. 5. 4. ... 9. 9. 5.]\n",
      "[9. 5. 4. ... 9. 9. 5.]\n",
      "[9. 5. 4. ... 9. 9. 5.]\n",
      "[9. 5. 4. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "[9. 2. 4. ... 9. 9. 5.]\n",
      "seed=080000, heterogeneity=57454.51852\n",
      "[8. 3. 6. ... 8. 8. 9.]\n",
      "[8. 3. 1. ... 8. 8. 2.]\n",
      "[8. 3. 1. ... 8. 8. 2.]\n",
      "[8. 3. 1. ... 8. 8. 2.]\n",
      "[8. 3. 1. ... 8. 8. 2.]\n",
      "[8. 3. 1. ... 8. 8. 3.]\n",
      "[8. 3. 1. ... 8. 8. 3.]\n",
      "[8. 3. 1. ... 8. 8. 3.]\n",
      "[8. 3. 1. ... 8. 8. 3.]\n",
      "[8. 3. 1. ... 8. 8. 3.]\n",
      "[8. 3. 1. ... 8. 8. 3.]\n",
      "[8. 3. 1. ... 8. 8. 3.]\n",
      "[8. 3. 1. ... 8. 8. 3.]\n",
      "[8. 3. 1. ... 8. 8. 3.]\n",
      "[8. 3. 1. ... 8. 8. 3.]\n",
      "[8. 3. 1. ... 8. 8. 3.]\n",
      "[8. 3. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "[8. 2. 1. ... 8. 8. 3.]\n",
      "seed=100000, heterogeneity=57471.56674\n",
      "[6. 3. 5. ... 6. 6. 9.]\n",
      "[6. 8. 4. ... 6. 6. 7.]\n",
      "[6. 8. 4. ... 6. 6. 7.]\n",
      "[6. 8. 4. ... 6. 6. 2.]\n",
      "[6. 8. 4. ... 6. 6. 2.]\n",
      "[6. 8. 4. ... 6. 6. 2.]\n",
      "[6. 8. 4. ... 6. 6. 2.]\n",
      "[6. 8. 4. ... 6. 6. 2.]\n",
      "[6. 8. 4. ... 6. 6. 2.]\n",
      "[6. 8. 4. ... 6. 6. 2.]\n",
      "[6. 8. 4. ... 6. 6. 2.]\n",
      "[6. 8. 4. ... 6. 6. 2.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "[6. 8. 4. ... 6. 6. 3.]\n",
      "seed=120000, heterogeneity=57523.28839\n",
      "CPU times: user 6min 13s, sys: 10.8 s, total: 6min 24s\n",
      "Wall time: 6min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "k = 10\n",
    "heterogeneity_smart = {}\n",
    "seeds = [0, 20000, 40000, 60000, 80000, 100000, 120000]\n",
    "for seed in seeds:\n",
    "    initial_centroids = smart_initialize(tf_idf, k, seed)\n",
    "    centroids, cluster_assignment = kmeans(tf_idf, k, initial_centroids, maxiter=400,\n",
    "                                           record_heterogeneity=None, verbose=False)\n",
    "    # To save time, compute heterogeneity only once in the end\n",
    "    heterogeneity_smart[seed] = compute_heterogeneity(tf_idf, k, centroids, cluster_assignment)\n",
    "    print('seed={0:06d}, heterogeneity={1:.5f}'.format(seed, heterogeneity_smart[seed]))\n",
    "    sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the set of cluster heterogeneities we got from our 7 restarts of k-means using random initialization compared to the 7 restarts of k-means using k-means++ as a smart initialization.\n",
    "\n",
    "The following code produces a [box plot](http://matplotlib.org/api/pyplot_api.html) for each of these methods, indicating the spread of values produced by each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.boxplot([list(heterogeneity.values()), list(heterogeneity_smart.values())], vert=False)\n",
    "plt.yticks([1, 2], ['k-means', 'k-means++'])\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few things to notice from the box plot:\n",
    "* On average, k-means++ produces a better clustering than Random initialization.\n",
    "* Variation in clustering quality is smaller for k-means++."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In general, you should run k-means at least a few times with different initializations and then return the run resulting in the lowest heterogeneity.** Let us write a function that runs k-means multiple times and picks the best run that minimizes heterogeneity. The function accepts an optional list of seed values to be used for the multiple runs; if no such list is provided, the current UTC time is used as seed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_multiple_runs(data, k, maxiter, num_runs, seed_list=None, verbose=False):\n",
    "    heterogeneity = []\n",
    "    \n",
    "    min_heterogeneity_achieved = float('inf')\n",
    "    best_seed = None\n",
    "    final_centroids = None\n",
    "    final_cluster_assignment = None\n",
    "    \n",
    "    for i in range(num_runs):\n",
    "        \n",
    "        # Use UTC time if no seeds are provided \n",
    "        if seed_list is not None: \n",
    "            seed = seed_list[i]\n",
    "            np.random.seed(seed)\n",
    "        else: \n",
    "            seed = int(time.time())\n",
    "            np.random.seed(seed)\n",
    "        \n",
    "        # Use k-means++ initialization\n",
    "        # YOUR CODE HERE\n",
    "        initial_centroids = smart_initialize(data,k,seed)\n",
    "        \n",
    "        # Run k-means\n",
    "        # YOUR CODE HERE\n",
    "        centroids, cluster_assignment = kmeans(tf_idf, k, initial_centroids, maxiter,\n",
    "                                      heterogeneity, verbose)\n",
    "        \n",
    "        # To save time, compute heterogeneity only once in the end\n",
    "        # YOUR CODE HERE\n",
    "        heterogeneity[seed] = compute_heterogeneity(data, k, centroids, cluster_assignment)\n",
    "        \n",
    "        if verbose:\n",
    "            print('seed={0:06d}, heterogeneity={1:.5f}'.format(seed, heterogeneity[seed]))\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        # if current measurement of heterogeneity is lower than previously seen,\n",
    "        # update the minimum record of heterogeneity.\n",
    "        if heterogeneity[seed] < min_heterogeneity_achieved:\n",
    "            min_heterogeneity_achieved = heterogeneity[seed]\n",
    "            best_seed = seed\n",
    "            final_centroids = centroids\n",
    "            final_cluster_assignment = cluster_assignment\n",
    "    \n",
    "    # Return the centroids and cluster assignments that minimize heterogeneity.\n",
    "    return final_centroids, final_cluster_assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to choose K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are measuring the tightness of the clusters, a higher value of K reduces the possible heterogeneity metric by definition.  For example, if we have N data points and set K=N clusters, then we could have 0 cluster heterogeneity by setting the N centroids equal to the values of the N data points. (Note: Not all runs for larger K will result in lower heterogeneity than a single run with smaller K due to local optima.)  Let's explore this general trend for ourselves by performing the following analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `kmeans_multiple_runs` function to run k-means with five different values of K.  For each K, use k-means++ and multiple runs to pick the best solution.  In what follows, we consider K=2,10,25,50,100 and 7 restarts for each setting.\n",
    "\n",
    "**IMPORTANT: The code block below will take about 10 minutes to finish**\n",
    "\n",
    "In order to speed up the computations, we run them with only one random seed, but for better performance, one should use more seeds and compare the results. If you don't mind running the code for approximately one hour, feel free to uncomment the following line of code below:\n",
    "\n",
    "`seed_list = [0]#, 20000, 40000, 60000, 80000, 100000, 120000]`\n",
    "\n",
    "Side note: In practice, a good implementation of k-means would utilize parallelism to run multiple runs of k-means at once. For an example, see [scikit-learn's KMeans](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[1. 0. 1. ... 1. 0. 1.]\n",
      "1\n",
      "[1. 0. 1. ... 1. 1. 0.]\n",
      "    14956 elements changed their cluster assignment.\n",
      "2\n",
      "[1. 0. 1. ... 1. 1. 0.]\n",
      "     3938 elements changed their cluster assignment.\n",
      "3\n",
      "[1. 0. 1. ... 1. 1. 0.]\n",
      "     1240 elements changed their cluster assignment.\n",
      "4\n",
      "[1. 0. 1. ... 1. 1. 0.]\n",
      "      424 elements changed their cluster assignment.\n",
      "5\n",
      "[1. 0. 1. ... 1. 1. 0.]\n",
      "      238 elements changed their cluster assignment.\n",
      "6\n",
      "[1. 0. 1. ... 1. 1. 0.]\n",
      "      154 elements changed their cluster assignment.\n",
      "7\n",
      "[1. 0. 1. ... 1. 1. 0.]\n",
      "      116 elements changed their cluster assignment.\n",
      "8\n",
      "[1. 0. 1. ... 1. 1. 0.]\n",
      "       73 elements changed their cluster assignment.\n",
      "9\n",
      "[1. 0. 1. ... 1. 1. 0.]\n",
      "       50 elements changed their cluster assignment.\n",
      "10\n",
      "[1. 0. 1. ... 1. 1. 0.]\n",
      "       43 elements changed their cluster assignment.\n",
      "11\n",
      "[1. 0. 1. ... 1. 1. 0.]\n",
      "       37 elements changed their cluster assignment.\n",
      "12\n",
      "[1. 0. 1. ... 1. 1. 0.]\n",
      "       23 elements changed their cluster assignment.\n",
      "13\n",
      "[1. 0. 1. ... 1. 1. 0.]\n",
      "       13 elements changed their cluster assignment.\n",
      "14\n",
      "[1. 0. 1. ... 1. 1. 0.]\n",
      "       10 elements changed their cluster assignment.\n",
      "15\n",
      "[1. 0. 1. ... 1. 1. 0.]\n",
      "        5 elements changed their cluster assignment.\n",
      "16\n",
      "[1. 0. 1. ... 1. 1. 0.]\n",
      "        6 elements changed their cluster assignment.\n",
      "17\n",
      "[1. 0. 1. ... 1. 1. 0.]\n",
      "        4 elements changed their cluster assignment.\n",
      "18\n",
      "[1. 0. 1. ... 1. 1. 0.]\n",
      "        3 elements changed their cluster assignment.\n",
      "19\n",
      "[1. 0. 1. ... 1. 1. 0.]\n",
      "        3 elements changed their cluster assignment.\n",
      "20\n",
      "[1. 0. 1. ... 1. 1. 0.]\n",
      "        1 elements changed their cluster assignment.\n",
      "21\n",
      "[1. 0. 1. ... 1. 1. 0.]\n",
      "seed=000000, heterogeneity=58224.59913\n",
      "0\n",
      "[2. 9. 6. ... 2. 2. 2.]\n",
      "1\n",
      "[2. 0. 6. ... 2. 2. 2.]\n",
      "    27819 elements changed their cluster assignment.\n",
      "2\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "    12656 elements changed their cluster assignment.\n",
      "3\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "     7091 elements changed their cluster assignment.\n",
      "4\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "     4578 elements changed their cluster assignment.\n",
      "5\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "     2828 elements changed their cluster assignment.\n",
      "6\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "     1536 elements changed their cluster assignment.\n",
      "7\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "      845 elements changed their cluster assignment.\n",
      "8\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "      522 elements changed their cluster assignment.\n",
      "9\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "      337 elements changed their cluster assignment.\n",
      "10\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "      279 elements changed their cluster assignment.\n",
      "11\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "      217 elements changed their cluster assignment.\n",
      "12\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "      208 elements changed their cluster assignment.\n",
      "13\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "      202 elements changed their cluster assignment.\n",
      "14\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "      211 elements changed their cluster assignment.\n",
      "15\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "      215 elements changed their cluster assignment.\n",
      "16\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "      244 elements changed their cluster assignment.\n",
      "17\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "      272 elements changed their cluster assignment.\n",
      "18\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "      283 elements changed their cluster assignment.\n",
      "19\n",
      "[2. 9. 6. ... 8. 2. 0.]\n",
      "      343 elements changed their cluster assignment.\n",
      "20\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "      351 elements changed their cluster assignment.\n",
      "21\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "      324 elements changed their cluster assignment.\n",
      "22\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "      305 elements changed their cluster assignment.\n",
      "23\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "      243 elements changed their cluster assignment.\n",
      "24\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "      229 elements changed their cluster assignment.\n",
      "25\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "      223 elements changed their cluster assignment.\n",
      "26\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "      223 elements changed their cluster assignment.\n",
      "27\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "      230 elements changed their cluster assignment.\n",
      "28\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "      212 elements changed their cluster assignment.\n",
      "29\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "      215 elements changed their cluster assignment.\n",
      "30\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "      223 elements changed their cluster assignment.\n",
      "31\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "      262 elements changed their cluster assignment.\n",
      "32\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "      284 elements changed their cluster assignment.\n",
      "33\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "      287 elements changed their cluster assignment.\n",
      "34\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "      311 elements changed their cluster assignment.\n",
      "35\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "      286 elements changed their cluster assignment.\n",
      "36\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "      220 elements changed their cluster assignment.\n",
      "37\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "      177 elements changed their cluster assignment.\n",
      "38\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "      109 elements changed their cluster assignment.\n",
      "39\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "       60 elements changed their cluster assignment.\n",
      "40\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "       46 elements changed their cluster assignment.\n",
      "41\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "       40 elements changed their cluster assignment.\n",
      "42\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "       48 elements changed their cluster assignment.\n",
      "43\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "       52 elements changed their cluster assignment.\n",
      "44\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "       39 elements changed their cluster assignment.\n",
      "45\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "       30 elements changed their cluster assignment.\n",
      "46\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "       16 elements changed their cluster assignment.\n",
      "47\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "        9 elements changed their cluster assignment.\n",
      "48\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "        7 elements changed their cluster assignment.\n",
      "49\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "        4 elements changed their cluster assignment.\n",
      "50\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "        3 elements changed their cluster assignment.\n",
      "51\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "        3 elements changed their cluster assignment.\n",
      "52\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "        2 elements changed their cluster assignment.\n",
      "53\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "        3 elements changed their cluster assignment.\n",
      "54\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "        5 elements changed their cluster assignment.\n",
      "55\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "        8 elements changed their cluster assignment.\n",
      "56\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "        5 elements changed their cluster assignment.\n",
      "57\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "        5 elements changed their cluster assignment.\n",
      "58\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "        3 elements changed their cluster assignment.\n",
      "59\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "        1 elements changed their cluster assignment.\n",
      "60\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "        1 elements changed their cluster assignment.\n",
      "61\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "        1 elements changed their cluster assignment.\n",
      "62\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "        1 elements changed their cluster assignment.\n",
      "63\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "        1 elements changed their cluster assignment.\n",
      "64\n",
      "[8. 9. 6. ... 8. 2. 0.]\n",
      "seed=000000, heterogeneity=57468.63808\n",
      "0\n",
      "[24. 24. 23. ... 22. 22.  2.]\n",
      "1\n",
      "[24.  0. 23. ... 22. 24. 13.]\n",
      "    26872 elements changed their cluster assignment.\n",
      "2\n",
      "[24.  0. 23. ... 22. 24. 13.]\n",
      "    11054 elements changed their cluster assignment.\n",
      "3\n",
      "[24.  0. 23. ... 22. 24. 13.]\n",
      "     6720 elements changed their cluster assignment.\n",
      "4\n",
      "[24.  0. 23. ... 22. 24. 13.]\n",
      "     4001 elements changed their cluster assignment.\n",
      "5\n",
      "[24.  0. 23. ... 22. 24. 13.]\n",
      "     2757 elements changed their cluster assignment.\n",
      "6\n",
      "[24.  0. 23. ... 22. 24. 13.]\n",
      "     1959 elements changed their cluster assignment.\n",
      "7\n",
      "[24.  0. 23. ... 22. 24. 13.]\n",
      "     1481 elements changed their cluster assignment.\n",
      "8\n",
      "[24.  8. 23. ... 22. 24. 13.]\n",
      "     1308 elements changed their cluster assignment.\n",
      "9\n",
      "[24.  8. 23. ... 22. 24. 13.]\n",
      "     1266 elements changed their cluster assignment.\n",
      "10\n",
      "[24.  8. 23. ... 22. 24. 13.]\n",
      "     1152 elements changed their cluster assignment.\n",
      "11\n",
      "[24.  8. 23. ... 22. 24. 13.]\n",
      "      937 elements changed their cluster assignment.\n",
      "12\n",
      "[24.  8. 23. ... 22. 24. 13.]\n",
      "      739 elements changed their cluster assignment.\n",
      "13\n",
      "[24.  8. 23. ... 22. 24. 13.]\n",
      "      582 elements changed their cluster assignment.\n",
      "14\n",
      "[24.  8. 23. ... 22. 24. 13.]\n",
      "      491 elements changed their cluster assignment.\n",
      "15\n",
      "[24.  8. 23. ... 22. 24. 13.]\n",
      "      468 elements changed their cluster assignment.\n",
      "16\n",
      "[24.  8. 23. ... 22. 24. 13.]\n",
      "      489 elements changed their cluster assignment.\n",
      "17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24.  8. 23. ... 22. 24. 13.]\n",
      "      527 elements changed their cluster assignment.\n",
      "18\n",
      "[24.  8. 23. ... 22. 24. 13.]\n",
      "      522 elements changed their cluster assignment.\n",
      "19\n",
      "[24.  8. 23. ... 22. 24. 13.]\n",
      "      364 elements changed their cluster assignment.\n",
      "20\n",
      "[24.  8. 23. ... 22. 24. 13.]\n",
      "      211 elements changed their cluster assignment.\n",
      "21\n",
      "[24.  8. 23. ... 22. 24. 13.]\n",
      "      152 elements changed their cluster assignment.\n",
      "22\n",
      "[24.  8. 23. ... 22. 24. 13.]\n",
      "      110 elements changed their cluster assignment.\n",
      "23\n",
      "[24.  8. 23. ... 22. 24. 13.]\n",
      "       92 elements changed their cluster assignment.\n",
      "24\n",
      "[24.  8. 23. ... 22. 24. 13.]\n",
      "       49 elements changed their cluster assignment.\n",
      "25\n",
      "[24.  8. 23. ... 22. 24. 13.]\n",
      "       30 elements changed their cluster assignment.\n",
      "26\n",
      "[24.  8. 23. ... 22. 24. 13.]\n",
      "       15 elements changed their cluster assignment.\n",
      "27\n",
      "[24.  8. 23. ... 22. 24. 13.]\n",
      "       14 elements changed their cluster assignment.\n",
      "28\n",
      "[24.  8. 23. ... 22. 24. 13.]\n",
      "       10 elements changed their cluster assignment.\n",
      "29\n",
      "[24.  8. 23. ... 22. 24. 13.]\n",
      "        8 elements changed their cluster assignment.\n",
      "30\n",
      "[24.  8. 23. ... 22. 24. 13.]\n",
      "        6 elements changed their cluster assignment.\n",
      "31\n",
      "[24.  8. 23. ... 22. 24. 13.]\n",
      "        4 elements changed their cluster assignment.\n",
      "32\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-8da9583c4eae>\u001b[0m in \u001b[0;36mkmeans_multiple_runs\u001b[0;34m(data, k, maxiter, num_runs, seed_list, verbose)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         centroids, cluster_assignment = kmeans(tf_idf, k, initial_centroids, maxiter,\n\u001b[0;32m---> 26\u001b[0;31m                                       heterogeneity, verbose)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# To save time, compute heterogeneity only once in the end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-24e760ac341c>\u001b[0m in \u001b[0;36mkmeans\u001b[0;34m(data, k, initial_centroids, maxiter, record_heterogeneity, verbose)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# 1. Make cluster assignments using nearest centroids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mcluster_assignment\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0massign_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# 2. Compute a new centroid for each of the k clusters, averaging all data points assigned to that cluster.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-c5c3423b685a>\u001b[0m in \u001b[0;36massign_clusters\u001b[0;34m(data, centroids)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Compute distances between each data point and the set of centroids:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Fill in the blank (RHS only)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdistances_from_centroids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'euclidean'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Compute cluster assignments for each data point:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/virtual_environment_name/lib/python3.7/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_parallel_pairwise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/virtual_environment_name/lib/python3.7/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meffective_n_jobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;31m# enforce a threading backend to prevent data communication overhead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/virtual_environment_name/lib/python3.7/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;31m# if dtype is already float64, no need to chunk and upcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mdistances\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mYY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/virtual_environment_name/lib/python3.7/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     if (sparse.issparse(a) and sparse.issparse(b)\n",
      "\u001b[0;32m~/anaconda3/envs/virtual_environment_name/lib/python3.7/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__matmul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    562\u001b[0m             raise ValueError(\"Scalar operands are not allowed, \"\n\u001b[1;32m    563\u001b[0m                              \"use '*' instead\")\n\u001b[0;32m--> 564\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__mul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rmatmul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/virtual_environment_name/lib/python3.7/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul_multivector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misscalarlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/virtual_environment_name/lib/python3.7/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m_mul_multivector\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sparsetools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_matvecs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         fn(M, N, n_vecs, self.indptr, self.indices, self.data,\n\u001b[0;32m--> 487\u001b[0;31m            other.ravel(), result.ravel())\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import numpy as np \n",
    "\n",
    "def plot_k_vs_heterogeneity(k_values, heterogeneity_values):\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(k_values, heterogeneity_values, linewidth=4)\n",
    "    plt.xlabel('K')\n",
    "    plt.ylabel('Heterogeneity')\n",
    "    plt.title('K vs. Heterogeneity')\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    plt.tight_layout()\n",
    "\n",
    "centroids = {}\n",
    "cluster_assignment = {}\n",
    "heterogeneity_values = []\n",
    "k_list = [2, 10, 25, 50, 100]\n",
    "seed_list = [0]\n",
    "# Uncomment the following line to run the plot with all the seeds (it may take about an hour to finish).\n",
    "#seed_list = [0, 20000, 40000, 60000, 80000, 100000, 120000]\n",
    "\n",
    "for k in k_list:\n",
    "    heterogeneity = []\n",
    "    centroids[k], cluster_assignment[k] = kmeans_multiple_runs(tf_idf, k, maxiter=400,\n",
    "                                                               num_runs=len(seed_list),                                                               seed_list=seed_list,\n",
    "                                                               verbose=True)\n",
    "    score = compute_heterogeneity(tf_idf, k, centroids[k], cluster_assignment[k])\n",
    "    heterogeneity_values.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_k_vs_heterogeneity(k_list, heterogeneity_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above plot we show that heterogeneity goes down as we increase the number of clusters. Does this mean we should always favor a higher K? **Not at all!** As we will see in the following section, setting K too high may end up separating data points that are actually pretty alike. At the extreme, we can set individual data points to be their own clusters (K=N) and achieve zero heterogeneity, but separating each data point into its own cluster is hardly a desirable outcome. In the following section, we will learn how to detect a K set \"too large\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize clusters of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start visualizing some clustering results to see if we think the clustering makes sense.  We can use such visualizations to help us assess whether we have set K too large or too small for a given application.  Following the theme of this course, we will judge whether the clustering makes sense in the context of document analysis.\n",
    "\n",
    "What are we looking for in a good clustering of documents?\n",
    "* Documents in the same cluster should be similar.\n",
    "* Documents from different clusters should be less similar.\n",
    "\n",
    "So a bad clustering exhibits either of two symptoms:\n",
    "* Documents in a cluster have mixed content.\n",
    "* Documents with similar content are divided up and put into different clusters.\n",
    "\n",
    "To help visualize the clustering, we do the following:\n",
    "* Fetch nearest neighbors of each centroid from the set of documents assigned to that cluster. We will consider these documents as being representative of the cluster.\n",
    "* Print titles and first sentences of those nearest neighbors.\n",
    "* Print top 5 words that have highest tf-idf weights in each centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_document_clusters(wiki, tf_idf, centroids, cluster_assignment, k, map_word_to_index, display_content=True):\n",
    "    '''wiki: original dataframe\n",
    "       tf_idf: data matrix, sparse matrix format\n",
    "       map_index_to_word: SFrame specifying the mapping betweeen words and column indices\n",
    "       display_content: if True, display 8 nearest neighbors of each centroid'''\n",
    "    map_index_to_word =  {v:k for k,v in map_word_to_index.items()}\n",
    "    print('==========================================================')\n",
    "    # Visualize each cluster c\n",
    "    for c in range(k):\n",
    "        # Cluster heading\n",
    "        print('Cluster {0:d}    '.format(c)),\n",
    "        # Print top 5 words with largest TF-IDF weights in the cluster\n",
    "        idx = centroids[c].argsort()[::-1]\n",
    "        for i in range(5): # Print each word along with the TF-IDF weight\n",
    "            print('{0:s}:{1:.3f}'.format(map_index_to_word[idx[i]], centroids[c][idx[i]])),\n",
    "        print('')\n",
    "        \n",
    "        if display_content:\n",
    "            # Compute distances from the centroid to all data points in the cluster,\n",
    "            # and compute nearest neighbors of the centroids within the cluster.\n",
    "            distances = pairwise_distances(tf_idf, centroids[c].reshape(1, -1), metric='euclidean').flatten()\n",
    "            distances[cluster_assignment!=c] = float('inf') # remove non-members from consideration\n",
    "            \n",
    "            nearest_neighbors = distances.argsort()\n",
    "            \n",
    "            # For 8 nearest neighbors, print the title as well as first 180 characters of text.\n",
    "            # Wrap the text at 80-character mark.\n",
    "            for i in range(8):\n",
    "                text = ' '.join(wiki[nearest_neighbors[i]]['text'].split(None, 25)[0:25])\n",
    "                print('\\n* {0:50s} {1:.5f}\\n  {2:s}\\n  {3:s}'.format(wiki[nearest_neighbors[i]]['name'],\n",
    "                    distances[nearest_neighbors[i]], text[:90], text[90:180] if len(text) > 90 else ''))\n",
    "        print('==========================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first look at the 2 cluster case (K=2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "Cluster 0    \n",
      "she:0.021\n",
      "university:0.015\n",
      "her:0.013\n",
      "he:0.012\n",
      "served:0.010\n",
      "\n",
      "\n",
      "* Kayee Griffin                                      0.97358\n",
      "  kayee frances griffin born 6 february 1950 is an australian politician and former australi\n",
      "  an labor party member of the new south wales legislative council serving\n",
      "\n",
      "* %C3%81ine Hyland                                   0.97370\n",
      "  ine hyland ne donlon is emeritus professor of education and former vicepresident of univer\n",
      "  sity college cork ireland she was born in 1942 in athboy co\n",
      "\n",
      "* Christine Robertson                                0.97373\n",
      "  christine mary robertson born 5 october 1948 is an australian politician and former austra\n",
      "  lian labor party member of the new south wales legislative council serving\n",
      "\n",
      "* Anita Kunz                                         0.97471\n",
      "  anita e kunz oc born 1956 is a canadianborn artist and illustratorkunz has lived in london\n",
      "   new york and toronto contributing to magazines and working\n",
      "\n",
      "* Barry Sullivan (lawyer)                            0.97488\n",
      "  barry sullivan is a chicago lawyer and as of july 1 2009 the cooney conway chair in advoca\n",
      "  cy at loyola university chicago school of law\n",
      "\n",
      "* Margaret Catley-Carlson                            0.97534\n",
      "  margaret catleycarlson oc born 6 october 1942 is a canadian civil servant she was chair an\n",
      "  d is now a patron of the global water partnership\n",
      "\n",
      "* Vanessa Gilmore                                    0.97579\n",
      "  vanessa diane gilmore born october 1956 is a judge on the united states district court for\n",
      "   the southern district of texas she was appointed to\n",
      "\n",
      "* James A. Joseph                                    0.97624\n",
      "  james a joseph born 1935 is an american former diplomatjoseph is professor of the practice\n",
      "   of public policy studies at duke university and founder of\n",
      "==========================================================\n",
      "Cluster 1    \n",
      "she:0.023\n",
      "music:0.017\n",
      "her:0.017\n",
      "league:0.016\n",
      "season:0.016\n",
      "\n",
      "\n",
      "* Patricia Scott                                     0.97143\n",
      "  patricia scott pat born july 14 1929 is a former pitcher who played in the allamerican gir\n",
      "  ls professional baseball league for parts of four seasons\n",
      "\n",
      "* Madonna (entertainer)                              0.97181\n",
      "  madonna louise ciccone tkoni born august 16 1958 is an american singer songwriter actress \n",
      "  and businesswoman she achieved popularity by pushing the boundaries of lyrical\n",
      "\n",
      "* Janet Jackson                                      0.97257\n",
      "  janet damita jo jackson born may 16 1966 is an american singer songwriter and actress know\n",
      "  n for a series of sonically innovative socially conscious and\n",
      "\n",
      "* Natashia Williams                                  0.97343\n",
      "  natashia williamsblach born august 2 1978 is an american actress and former wonderbra camp\n",
      "  aign model who is perhaps best known for her role as shane\n",
      "\n",
      "* Todd Williams                                      0.97384\n",
      "  todd michael williams born february 13 1971 in syracuse new york is a former major league \n",
      "  baseball relief pitcher he attended east syracuseminoa high school\n",
      "\n",
      "* Marilyn Jenkins                                    0.97430\n",
      "  marilyn a jenkins jenks born september 18 1934 is a former catcher who played in the allam\n",
      "  erican girls professional baseball league listed at 5 ft\n",
      "\n",
      "* Kayla Bashore Smedley                              0.97496\n",
      "  kayla bashore born february 20 1983 in daegu south korea is an american field hockey defen\n",
      "  der and midfielder now living in san diego california she\n",
      "\n",
      "* Cher                                               0.97510\n",
      "  cher r born cherilyn sarkisian may 20 1946 is an american singer actress and television ho\n",
      "  st described as embodying female autonomy in a maledominated industry\n",
      "==========================================================\n"
     ]
    }
   ],
   "source": [
    "'''Notice the extra pairs of parentheses for centroids and cluster_assignment.\n",
    "   The centroid and cluster_assignment are still inside the npz file,\n",
    "   and we need to explicitly indicate when to load them into memory.'''\n",
    "visualize_document_clusters(wiki, tf_idf, centroids[2], cluster_assignment[2], 2, map_index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both clusters have mixed content, although cluster 1 is much purer than cluster 0:\n",
    "* Cluster 0: academia, law\n",
    "* Cluster 1: female figures, baseball players\n",
    "\n",
    "Roughly speaking, the entire dataset was divided into athletes and non-athletes. It would be better if we sub-divided non-atheletes into more categories. So let us use more clusters. How about `K=10`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================\n",
      "Cluster 0    \n",
      "he:0.012\n",
      "art:0.011\n",
      "his:0.009\n",
      "book:0.008\n",
      "that:0.008\n",
      "\n",
      "\n",
      "* Wilson McLean                                      0.97661\n",
      "  wilson mclean born 1937 is a scottish illustrator and artist he has illustrated primarily \n",
      "  in the field of advertising but has also provided cover art\n",
      "\n",
      "* Tang Xiyang                                        0.97988\n",
      "  tang xiyang born january 30 1930 in miluo hunan province is a chinese environmentalist he \n",
      "  was awarded the 2007 ramon magsaysay award for peace and\n",
      "\n",
      "* David Salle                                        0.98168\n",
      "  david salle born 1952 is an american painter printmaker and stage designer who helped defi\n",
      "  ne postmodern sensibility salle was born in norman oklahoma he earned\n",
      "\n",
      "* Alberto Blanco (poet)                              0.98172\n",
      "  alberto blanco is considered one of mexicos most important poets born in mexico city on fe\n",
      "  bruary 18 1951 he spent his childhood and adolescence in\n",
      "\n",
      "* John Donald (jewellery designer)                   0.98290\n",
      "  john donald is a british jeweller designer whose work is strongly identified in the 1960s \n",
      "  and 1970s in london princess margaret and the queen mother\n",
      "\n",
      "* David Elliott (curator)                            0.98298\n",
      "  david stuart elliott born 29 april 1949 is a britishborn art gallery and museum curator an\n",
      "  d writer about modern and contemporary arthe was educated at\n",
      "\n",
      "* Chris Hunt                                         0.98307\n",
      "  chris hunt is a british journalist magazine editor and author he has worked in journalism \n",
      "  for over twenty years most often writing about football or\n",
      "\n",
      "* Kcho                                               0.98342\n",
      "  kchosometimes spelled kcho born alexis leiva machado on the isla de pinos 1970 is a contem\n",
      "  porary cuban artist kcho has had art showings around the\n",
      "==========================================================\n",
      "Cluster 1    \n",
      "film:0.088\n",
      "theatre:0.037\n",
      "films:0.032\n",
      "television:0.028\n",
      "actor:0.027\n",
      "\n",
      "\n",
      "* Shona Auerbach                                     0.93531\n",
      "  shona auerbach is a british film director and cinematographerauerbach began her career as \n",
      "  a stills photographer she studied film at manchester university and cinematography at\n",
      "\n",
      "* Singeetam Srinivasa Rao                            0.93748\n",
      "  singeetam srinivasa rao born 21 september 1931 is an indian film director producer screenw\n",
      "  riter composer singer lyricist and actor known for his works in telugu\n",
      "\n",
      "* Justin Edgar                                       0.93801\n",
      "  justin edgar is a british film directorborn in handsworth birmingham on 18 august 1971 edg\n",
      "  ar graduated from portsmouth university in 1996 with a first class\n",
      "\n",
      "* Laura Neri                                         0.94151\n",
      "  laura neri greek is a director of greek and italian origins born in brussels belgium she g\n",
      "  raduated from the usc school of cinematic arts in\n",
      "\n",
      "* Bill Bennett (director)                            0.94260\n",
      "  bill bennett born 1953 is an australian film director producer and screenwriterhe dropped \n",
      "  out of medicine at queensland university in 1972 and joined the australian\n",
      "\n",
      "* Robert Braiden                                     0.94344\n",
      "  robert braiden is an australian film director and writer born in sydney he grew up in moor\n",
      "  ebank liverpool new south wales and now currently lives\n",
      "\n",
      "* Nitzan Gilady                                      0.94369\n",
      "  nitzan gilady also known as nitzan giladi hebrew is an israeli film director who has writt\n",
      "  en produced and directed the documentary films in satmar custody\n",
      "\n",
      "* Robb Moss                                          0.94484\n",
      "  robb moss is an independent documentary filmmaker and professor at harvard university nota\n",
      "  ble work includes such films as the same river twice secrecy film and\n",
      "==========================================================\n",
      "Cluster 2    \n",
      "league:0.061\n",
      "baseball:0.048\n",
      "season:0.046\n",
      "coach:0.042\n",
      "games:0.034\n",
      "\n",
      "\n",
      "* Todd Williams                                      0.92759\n",
      "  todd michael williams born february 13 1971 in syracuse new york is a former major league \n",
      "  baseball relief pitcher he attended east syracuseminoa high school\n",
      "\n",
      "* Justin Knoedler                                    0.93295\n",
      "  justin joseph knoedler born july 17 1980 in springfield illinois is a former major league \n",
      "  baseball catcherknoedler was originally drafted by the st louis cardinals\n",
      "\n",
      "* Kevin Nicholson (baseball)                         0.93579\n",
      "  kevin ronald nicholson born march 29 1976 is a canadian baseball shortstop he played part \n",
      "  of the 2000 season for the san diego padres of\n",
      "\n",
      "* Dave Ford                                          0.93642\n",
      "  david alan ford born december 29 1956 is a former major league baseball pitcher for the ba\n",
      "  ltimore orioles born in cleveland ohio ford attended lincolnwest\n",
      "\n",
      "* Steve Springer                                     0.93649\n",
      "  steven michael springer born february 11 1961 is an american former professional baseball \n",
      "  player who appeared in major league baseball as a third baseman and\n",
      "\n",
      "* Chris Young (pitcher)                              0.93772\n",
      "  christopher ryan chris young born may 25 1979 is an american professional baseball rightha\n",
      "  nded pitcher who is a free agent he made his major league\n",
      "\n",
      "* Eric Fox                                           0.93875\n",
      "  eric hollis fox born august 15 1963 in lemoore california is an american professional base\n",
      "  ball coach the 5 ft 10 in 178 m 180 lb\n",
      "\n",
      "* Ted Silva                                          0.93948\n",
      "  theodore a silva born august 4 1974 in inglewood california has held numerous roles in ama\n",
      "  teur and professional baseball he has played in the minor\n",
      "==========================================================\n",
      "Cluster 3    \n",
      "party:0.046\n",
      "election:0.042\n",
      "minister:0.039\n",
      "elected:0.028\n",
      "member:0.020\n",
      "\n",
      "\n",
      "* Stephen Harper                                     0.95128\n",
      "  stephen joseph harper pc mp born april 30 1959 is a canadian politician who is the 22nd an\n",
      "  d current prime minister of canada and the\n",
      "\n",
      "* Lucienne Robillard                                 0.95307\n",
      "  lucienne robillard pc born june 16 1945 is a canadian politician and a member of the liber\n",
      "  al party of canada she sat in the house\n",
      "\n",
      "* Marcelle Mersereau                                 0.95379\n",
      "  marcelle mersereau born february 14 1942 in pointeverte new brunswick is a canadian politi\n",
      "  cian a civil servant for most of her career she also served\n",
      "\n",
      "* Maureen Lyster                                     0.95450\n",
      "  maureen anne lyster born 10 september 1943 is an australian politician she was an australi\n",
      "  an labor party member of the victorian legislative assembly from 1985\n",
      "\n",
      "* Bruce Flegg                                        0.95554\n",
      "  dr bruce stephen flegg born 10 march 1954 in sydney is an australian former politician he \n",
      "  was a member of the queensland legislative assembly from\n",
      "\n",
      "* Doug Lewis                                         0.95583\n",
      "  douglas grinslade doug lewis pc qc born april 17 1938 is a former canadian politician a ch\n",
      "  artered accountant and lawyer by training lewis entered the\n",
      "\n",
      "* Paul Martin                                        0.95583\n",
      "  paul edgar philippe martin pc cc born august 28 1938 also known as paul martin jr is a can\n",
      "  adian politician who was the 21st prime\n",
      "\n",
      "* Gordon Gibson                                      0.95612\n",
      "  gordon gibson obc born 1937 is a political columnist author and former politician in briti\n",
      "  sh columbia bc canada he is the son of the late\n",
      "==========================================================\n",
      "Cluster 4    \n",
      "music:0.095\n",
      "orchestra:0.087\n",
      "symphony:0.057\n",
      "opera:0.050\n",
      "conductor:0.041\n",
      "\n",
      "\n",
      "* Heiichiro Ohyama                                   0.89162\n",
      "  heiichiro ohyama yama heiichir born 1947 in kyoto japan is a japanese conductor and violis\n",
      "  the has a longestablished reputation as a remarkable conductor and one\n",
      "\n",
      "* Brenton Broadstock                                 0.90284\n",
      "  brenton broadstock ao born 1952 is an australian composerbroadstock was born in melbourne \n",
      "  he studied history politics and music at monash university and later composition\n",
      "\n",
      "* Toshiyuki Shimada                                  0.90301\n",
      "  toshiyuki shimada is a japanese american orchestral conductor he is music director of both\n",
      "   the eastern connecticut symphony orchestra in new london ct and the\n",
      "\n",
      "* David Porcelijn                                    0.90357\n",
      "  david porcelijn born 7 january 1947 in achtkarspelen is a dutch composer and conductordavi\n",
      "  d porcelijn studied flute composition and conducting at the royal conservatoire of\n",
      "\n",
      "* Hugh Wolff                                         0.90731\n",
      "  hugh wolff born 21 october 1953 in paris is an american conductorhe was born in paris whil\n",
      "  e his father was serving in the u s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* Daniel Meyer (conductor)                           0.90849\n",
      "  daniel meyer was born in cleveland ohio and has been conductor and musical director of sev\n",
      "  eral prominent american orchestrashe is a graduate of denison university\n",
      "\n",
      "* Paul Hostetter                                     0.90967\n",
      "  paul hostetter is the ethel foley distinguished chair in orchestral activities for the sch\n",
      "  wob school of music at columbus state university the conductor and artistic\n",
      "\n",
      "* Peter Ruzicka                                      0.91097\n",
      "  peter ruzicka born july 3 1948 is a german composer and conductor of classical musicpeter \n",
      "  ruzicka was born in dsseldorf on july 3 1948 he\n",
      "==========================================================\n",
      "Cluster 5    \n",
      "she:0.140\n",
      "her:0.088\n",
      "miss:0.012\n",
      "actress:0.011\n",
      "womens:0.011\n",
      "\n",
      "\n",
      "* Lauren Royal                                       0.93427\n",
      "  lauren royal born march 3 circa 1965 is a book writer from california royal has written bo\n",
      "  th historic and novelistic booksa selfproclaimed angels baseball fan\n",
      "\n",
      "* Janine Shepherd                                    0.93681\n",
      "  janine lee shepherd am born 1962 is an australian pilot and former crosscountry skier shep\n",
      "  herds career as an athlete ended when she suffered major injuries\n",
      "\n",
      "* Barbara Hershey                                    0.93708\n",
      "  barbara hershey born barbara lynn herzstein february 5 1948 once known as barbara seagull \n",
      "  is an american actress in a career spanning nearly 50 years\n",
      "\n",
      "* Janet Jackson                                      0.93735\n",
      "  janet damita jo jackson born may 16 1966 is an american singer songwriter and actress know\n",
      "  n for a series of sonically innovative socially conscious and\n",
      "\n",
      "* Ellina Graypel                                     0.93837\n",
      "  ellina graypel born july 19 1972 is an awardwinning russian singersongwriter she was born \n",
      "  near the volga river in the heart of russia she spent\n",
      "\n",
      "* Alexandra Potter                                   0.93867\n",
      "  alexandra potter born 1970 is a british author of romantic comediesborn in bradford yorksh\n",
      "  ire england and educated at liverpool university gaining an honors degree in\n",
      "\n",
      "* Dorothy E. Smith                                   0.93904\n",
      "  dorothy edith smithborn july 6 1926 is a canadian sociologist with research interests besi\n",
      "  des in sociology in many disciplines including womens studies psychology and educational\n",
      "\n",
      "* Jane Fonda                                         0.93914\n",
      "  jane fonda born lady jayne seymour fonda december 21 1937 is an american actress writer po\n",
      "  litical activist former fashion model and fitness guru she is\n",
      "==========================================================\n",
      "Cluster 6    \n",
      "album:0.053\n",
      "band:0.044\n",
      "music:0.042\n",
      "released:0.028\n",
      "jazz:0.023\n",
      "\n",
      "\n",
      "* Will.i.am                                          0.95336\n",
      "  william adams born march 15 1975 known by his stage name william pronounced will i am is a\n",
      "  n american rapper songwriter entrepreneur actor dj record\n",
      "\n",
      "* Tony Mills (musician)                              0.95359\n",
      "  tony mills born 7 july 1962 in solihull england is an english rock singer best known for h\n",
      "  is work with shy and tnthailing from birmingham\n",
      "\n",
      "* Keith Urban                                        0.95379\n",
      "  keith lionel urban born 26 october 1967 is a new zealand born australian country music sin\n",
      "  ger songwriter guitarist entrepreneur and music competition judge in 1991\n",
      "\n",
      "* Prince (musician)                                  0.95420\n",
      "  prince rogers nelson born june 7 1958 known by his mononym prince is an american singerson\n",
      "  gwriter multiinstrumentalist and actor he has produced ten platinum albums\n",
      "\n",
      "* Steve Overland                                     0.95503\n",
      "  steve overland is a british singermusician who was the lead vocalist and songwriter for th\n",
      "  e bands wildlife fm the ladder shadowman and his own group\n",
      "\n",
      "* Jesse Johnson (musician)                           0.95606\n",
      "  jesse woods johnson born june 1 1960 in rock island illinois is a musician best known as t\n",
      "  he guitarist in the original lineup of the\n",
      "\n",
      "* Mark Cross (musician)                              0.95609\n",
      "  mark cross born 2 august 1965 london is a hard rock and heavy metal drummer he was born to\n",
      "   an english father and german mother\n",
      "\n",
      "* Stewart Levine                                     0.95888\n",
      "  stewart levine is an american record producer he has worked with such artists as the crusa\n",
      "  ders minnie riperton lionel richie simply red hugh masekela dr\n",
      "==========================================================\n",
      "Cluster 7    \n",
      "law:0.133\n",
      "court:0.081\n",
      "judge:0.060\n",
      "district:0.042\n",
      "justice:0.040\n",
      "\n",
      "\n",
      "* Barry Sullivan (lawyer)                            0.89228\n",
      "  barry sullivan is a chicago lawyer and as of july 1 2009 the cooney conway chair in advoca\n",
      "  cy at loyola university chicago school of law\n",
      "\n",
      "* William G. Young                                   0.89433\n",
      "  william glover young born 1940 is a united states federal judge for the district of massac\n",
      "  husetts young was born in huntington new york he attended\n",
      "\n",
      "* Bernard Bell (attorney)                            0.89617\n",
      "  bernard bell is the associate dean for academic affairs and faculty professor of law and h\n",
      "  erbert hannoch scholar at rutgers school of lawnewark bell received\n",
      "\n",
      "* George B. Daniels                                  0.89796\n",
      "  george benjamin daniels born 1953 is a united states federal judge for the united states d\n",
      "  istrict court for the southern district of new yorkdaniels was\n",
      "\n",
      "* Robinson O. Everett                                0.90432\n",
      "  robinson o everett march 18 1928 june 12 2009 was an american lawyer judge and a professor\n",
      "   of law at duke universityeverett was born in\n",
      "\n",
      "* James G. Carr                                      0.90595\n",
      "  james g carr born july 7 1940 is a federal district judge for the united states district c\n",
      "  ourt for the northern district of ohiocarr was\n",
      "\n",
      "* John C. Eastman                                    0.90764\n",
      "  john c eastman born april 21 1960 is a conservative american law professor and constitutio\n",
      "  nal law scholar he is the henry salvatori professor of law\n",
      "\n",
      "* Jean Constance Hamilton                            0.90830\n",
      "  jean constance hamilton born 1945 is a senior united states district judge of the united s\n",
      "  tates district court for the eastern district of missouriborn in\n",
      "==========================================================\n",
      "Cluster 8    \n",
      "football:0.050\n",
      "league:0.044\n",
      "club:0.043\n",
      "season:0.042\n",
      "played:0.037\n",
      "\n",
      "\n",
      "* Chris Day                                          0.93686\n",
      "  christopher nicholas chris day born 28 july 1975 is an english professional footballer who\n",
      "   plays as a goalkeeper for stevenageday started his career at tottenham\n",
      "\n",
      "* Jason Roberts (footballer)                         0.93775\n",
      "  jason andre davis roberts mbe born 25 january 1978 is a former professional footballer and\n",
      "   now a football punditborn in park royal london roberts was\n",
      "\n",
      "* Tony Smith (footballer, born 1957)                 0.93839\n",
      "  anthony tony smith born 20 february 1957 is a former footballer who played as a central de\n",
      "  fender in the football league in the 1970s and\n",
      "\n",
      "* Neil Grayson                                       0.94178\n",
      "  neil grayson born 1 november 1964 in york is an english footballer who last played as a st\n",
      "  riker for sutton towngraysons first club was local\n",
      "\n",
      "* Richard Ambrose                                    0.94220\n",
      "  richard ambrose born 10 june 1972 is a former australian rules footballer who played with \n",
      "  the sydney swans in the australian football league afl he\n",
      "\n",
      "* Paul Robinson (footballer, born 1979)              0.94245\n",
      "  paul william robinson born 15 october 1979 is an english professional footballer who plays\n",
      "   for blackburn rovers as a goalkeeper he is a former england\n",
      "\n",
      "* Alex Lawless                                       0.94269\n",
      "  alexander graham alex lawless born 26 march 1985 is a welsh professional footballer who pl\n",
      "  ays for luton town as a midfielderlawless began his career with\n",
      "\n",
      "* Sol Campbell                                       0.94275\n",
      "  sulzeer jeremiah sol campbell born 18 september 1974 is a former england international foo\n",
      "  tballer a central defender he had a 19year career playing in the\n",
      "==========================================================\n",
      "Cluster 9    \n",
      "research:0.038\n",
      "university:0.035\n",
      "professor:0.030\n",
      "science:0.023\n",
      "institute:0.019\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "* Lawrence W. Green                                  0.95862\n",
      "  lawrence w green is best known by health education researchers as the originator of the pr\n",
      "  ecede model and codeveloper of the precedeproceed model which has\n",
      "\n",
      "* Timothy Luke                                       0.96028\n",
      "  timothy w luke is university distinguished professor of political science in the college o\n",
      "  f liberal arts and human sciences as well as program chair of\n",
      "\n",
      "* Ren%C3%A9e Fox                                     0.96119\n",
      "  rene c fox a summa cum laude graduate of smith college in 1949 earned her phd in sociology\n",
      "   in 1954 from radcliffe college harvard university\n",
      "\n",
      "* Francis Gavin                                      0.96213\n",
      "  francis j gavin is first frank stanton chair in nuclear security policy studies and profes\n",
      "  sor of political science at mit before joining mit he was\n",
      "\n",
      "* Catherine Hakim                                    0.96358\n",
      "  catherine hakim born 30 may 1948 is a british sociologist who specialises in womens employ\n",
      "  ment and womens issues she is currently a professorial research fellow\n",
      "\n",
      "* Daniel Berg (educator)                             0.96361\n",
      "  daniel berg is a scientist educator and was the fifteenth president of rensselaer polytech\n",
      "  nic institutehe was born on june 1 1929 in new york city\n",
      "\n",
      "* Georg von Krogh                                    0.96375\n",
      "  georg von krogh was born in oslo norway he is a professor at eth zurich and holds the chai\n",
      "  r of strategic management and innovation he\n",
      "\n",
      "* Martin Apple                                       0.96381\n",
      "  martin a apple is president of the council of scientific society presidents cssp an organi\n",
      "  zation of presidents of some sixty scientific federations and societies whose\n",
      "==========================================================\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "visualize_document_clusters(wiki, tf_idf, centroids[k], cluster_assignment[k], k, map_index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusters 0, 1, and 5 appear to be still mixed, but others are quite consistent in content.\n",
    "* Cluster 0: artists, book, him/his\n",
    "* Cluster 1: film, theatre, films, tv, actor \n",
    "* Cluster 2: baseball players\n",
    "* Cluster 3: elections, ministers\n",
    "* Cluster 4: music, orchestra, symphony \n",
    "* Cluster 5: female figures from various fields\n",
    "* Cluster 6: composers, songwriters, singers, music producers\n",
    "* Cluster 7: law, courts, justice \n",
    "* Cluster 8: football \n",
    "* Cluster 9: academia\n",
    "\n",
    "Clusters are now more pure, but some are qualitatively \"bigger\" than others. For instance, the category of scholars is more general than the category of baseball players. Increasing the number of clusters may split larger clusters. Another way to look at the size of the clusters is to count the number of articles in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19618,  3857,  4173,  5219,  1743,  6900,  5530,  1348,  4384,\n",
       "        6299])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(cluster_assignment[10].astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**. Which of the 10 clusters above contains the greatest number of articles?\n",
    "\n",
    "1. * Cluster 0: artists, book, him/his\n",
    "2. * Cluster 4: music, orchestra, symphony \n",
    "3. * Cluster 5: female figures from various fields\n",
    "4. * Cluster 7: law, courts, justice \n",
    "5. * Cluster 9: academia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**. Which of the 10 clusters contains the least number of articles?\n",
    "\n",
    "1. * Cluster 1: film, theatre, films, tv, actor \n",
    "2. * Cluster 3: elections, ministers\n",
    "3. * Cluster 6: composers, songwriters, singers, music producers\n",
    "4. * Cluster 7: law, courts, justice \n",
    "5. * Cluster 8: football "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be at least some connection between the topical consistency of a cluster and the number of its member data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the case for K=25. For the sake of brevity, we do not print the content of documents. It turns out that the top words with highest TF-IDF weights in each cluster are representative of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "25",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-6bf9582b0579>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#                             map_index_to_word, display_content=False) # turn off text for brevity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# visualize_document_clusters(wiki, tf_idf, centroids[25], cluster_assignment[25], 25, map_index_to_word)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mvisualize_document_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwiki\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_idf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcentroids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_assignment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_index_to_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 25"
     ]
    }
   ],
   "source": [
    "# visualize_document_clusters(wiki, tf_idf, centroids[25], cluster_assignment[25], 25,\n",
    "#                             map_index_to_word, display_content=False) # turn off text for brevity\n",
    "# visualize_document_clusters(wiki, tf_idf, centroids[25], cluster_assignment[25], 25, map_index_to_word)\n",
    "visualize_document_clusters(wiki, tf_idf, centroids[25], cluster_assignment[25], 25, map_index_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the representative examples and top words, we classify each cluster as follows.\n",
    "\n",
    "* Cluster 0: Literature\n",
    "* Cluster 1: Film and theater\n",
    "* Cluster 2: Law\n",
    "* Cluster 3: Politics\n",
    "* Cluster 4: Classical music\n",
    "* Cluster 5: Popular music\n",
    "* Cluster 6: Jazz music\n",
    "* Cluster 7: Business and economics\n",
    "* Cluster 8: (mixed; no clear theme)\n",
    "* Cluster 9: Academia and research\n",
    "* Cluster 10: International affairs\n",
    "* Cluster 11: Baseball\n",
    "* Cluster 12: Art\n",
    "* Cluster 13: Military\n",
    "* Cluster 14: Politics\n",
    "* Cluster 15: Radio and TV\n",
    "* Cluster 16: Catholic church\n",
    "* Cluster 17: Opera and ballet\n",
    "* Cluster 18: Orchestra music\n",
    "* Cluster 19: Females from various fields\n",
    "* Cluster 20: Car racing\n",
    "* Cluster 21: General sports\n",
    "* Cluster 22: Rugby\n",
    "* Cluster 23: Rock music\n",
    "* Cluster 24: Team sports\n",
    "\n",
    "Indeed, increasing K achieved the desired effect of breaking up large clusters.  Depending on the application, this may or may not be preferable to the K=10 analysis.\n",
    "\n",
    "Let's take it to the extreme and set K=100. We have a suspicion that this value is too large. Let us look at the top words from each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-11f74cffa0bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m visualize_document_clusters(wiki, tf_idf, centroids[k], cluster_assignment[k], k,\n\u001b[0m\u001b[1;32m      3\u001b[0m                             map_index_to_word, display_content=False)\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# turn off text for brevity -- turn it on if you are curious ;)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 100"
     ]
    }
   ],
   "source": [
    "k=100\n",
    "visualize_document_clusters(wiki, tf_idf, centroids[k], cluster_assignment[k], k,\n",
    "                            map_index_to_word, display_content=False)\n",
    "# turn off text for brevity -- turn it on if you are curious ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class of team sports has been broken into several clusters, soccer (association football) (11, 22, 24), rugby (76), hockey (80), basketball (86), cricket (87), and American football (85).\n",
    "\n",
    "The class of baseball has been broken into San Francisco Giants (45), baseball (61, 74), and baseball stats (88).\n",
    "\n",
    "The class of car racing has been broken into Nascar (20) and Formula 1 (52).\n",
    "\n",
    "**A high value of K encourages pure clusters, but we cannot keep increasing K. For large enough K, related documents end up going to different clusters.**\n",
    "\n",
    "That said, the result for K=100 is not entirely bad. After all, it gives us separate clusters for such categories as Brazil, wrestling, computer science and the Mormon Church. If we set K somewhere between 25 and 100, we should be able to avoid breaking up clusters while discovering new ones.\n",
    "\n",
    "Also, we should ask ourselves how much **granularity** we want in our clustering. If we wanted a rough sketch of Wikipedia, we don't want too detailed clusters. On the other hand, having many clusters can be valuable when we are zooming into a certain part of Wikipedia.\n",
    "\n",
    "**There is no golden rule for choosing K. It all depends on the particular application and domain we are in.**\n",
    "\n",
    "Another heuristic people use that does not rely on so much visualization, which can be hard in many applications (including here!) is as follows.  Track heterogeneity versus K and look for the \"elbow\" of the curve where the heterogeneity decrease rapidly before this value of K, but then only gradually for larger values of K.  This naturally trades off between trying to minimize heterogeneity, but reduce model complexity.  In the heterogeneity versus K plot made above, we did not yet really see a flattening out of the heterogeneity, which might indicate that indeed K=100 is \"reasonable\" and we only see real overfitting for larger values of K (which are even harder to visualize using the methods we attempted above.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Quiz Question**. Another sign of too large K is having lots of small clusters. Look at the distribution of cluster sizes (by number of member data points). How many of the 100 clusters have fewer than 236 articles, i.e. 0.4% of the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaway\n",
    "\n",
    "Keep in mind though that tiny clusters aren't necessarily bad. A tiny cluster of documents that really look like each others is definitely preferable to a medium-sized cluster of documents with mixed content. However, having too few articles in a cluster may cause overfitting by reading too much into a limited pool of training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
